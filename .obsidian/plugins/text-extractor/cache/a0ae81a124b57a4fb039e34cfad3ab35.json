{"path":"06. 공부 자료/pdf/book/Distributed_Systems_3rd 1 1.pdf","text":"Chapter 6 Coordination In the previous chapters, we have looked at processes and communication between processes. While communication is important, it is not the entire story. Closely related is how processes cooperate and synchronize with one another. Cooperation is partly supported by means of naming, which allows processes to at least share resources, or entities in general. In this chapter, we mainly concentrate on how processes can synchronize and coordinate their actions. For example, it is important that multiple processes do not simultaneously access a shared resource, such as a ﬁle, but instead cooperate in granting each other temporary exclusive access. Another example is that multiple processes may sometimes need to agree on the ordering of events, such as whether message m1 from process P was sent before or after message m2 from process Q. Synchronization and coordination are two closely related phenomena. In process synchronization we make sure that one process waits for another to complete its operation. When dealing with data synchronization, the problem is to ensure that two sets of data are the same. When it comes to coordination, the goal is to manage the interactions and dependencies between activities in a distributed system [Malone and Crowston, 1994]. From this perspective, one could state that coordination encapsulates synchronization. As it turns out, coordination in distributed systems is often much more difﬁcult compared to that in uniprocessor or multiprocessor systems. The problems and solutions that are discussed in this chapter are, by their nature, rather general, and occur in many different situations in distributed systems. We start with a discussion of the issue of synchronization based on actual time, followed by synchronization in which only relative ordering matters rather than ordering in absolute time. In many cases, it is important that a group of processes can appoint one process as a coordinator, which can be done by means of election algorithms. We discuss various election algorithms in a separate section. Before that, 297 298 CHAPTER 6. COORDINATION we look into a number of algorithms for coordinating mutual exclusion to a shared resource. As a special class of coordination problems, we also dive into location systems by which we place a process in a multidimensional plane. Such placements come in handy when dealing with very large distributed systems. We already came across publish-subscribe systems, but have not yet dis- cussed in any detail how we actually match subscriptions to publications. There are many ways to do this and we look at centralized as well as decen- tralized implementations. Finally, we consider three different gossip-based coordination problems: aggregation, peer sampling, and overlay construction. Distributed algorithms come in all sorts and ﬂavors and have been devel- oped for very different types of distributed systems. Many examples (and further references) can be found in Andrews [2000], Cachin et al. [2011], and Fokkink [2013]. More formal approaches to a wealth of algorithms can be found in text books from Attiya and Welch [2004], Lynch [1996], Santoro [2007], and Tel [2000]. 6.1 Clock synchronization In a centralized system, time is unambiguous. When a process wants to know the time, it simply makes a call to the operating system. If process A asks for the time, and then a little later process B asks for the time, the value that B gets will be higher than (or possibly equal to) the value A got. It will certainly not be lower. In a distributed system, achieving agreement on time is not trivial. Just think, for a moment, about the implications of the lack of global time on the Unix make program, as a simple example. Normally, in Unix large programs are split up into multiple source ﬁles, so that a change to one source ﬁle requires only one ﬁle to be recompiled, not all the ﬁles. If a program consists of 100 ﬁles, not having to recompile everything because one ﬁle has been changed greatly increases the speed at which programmers can work. The way make normally works is simple. When the programmer has ﬁnished changing all the source ﬁles, he runs make, which examines the times at which all the source and object ﬁles were last modiﬁed. If the source ﬁle input.c has time 2151 and the corresponding object ﬁle input.o has time 2150, make knows that input.c has been changed since input.o was created, and thus input.c must be recompiled. On the other hand, if output.c has time 2144 and output.o has time 2145, no compilation is needed. Thus make goes through all the source ﬁles to ﬁnd out which ones need to be recompiled and calls the compiler to recompile them. Now imagine what could happen in a distributed system in which there was no global agreement on time. Suppose that output.o has time 2144 as DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 299 above, and shortly thereafter output.c is modiﬁed but is assigned time 2143 because the clock on its machine is slightly behind, as shown in Figure 6.1. Make will not call the compiler. The resulting executable binary program will then contain a mixture of object ﬁles from the old sources and the new sources. It will probably crash and the programmer will go crazy trying to understand what is wrong with the code. Figure 6.1: When each machine has its own clock, an event that occurred after another event may nevertheless be assigned an earlier time. There are many more examples where an accurate account of time is needed. The example above can easily be reformulated to ﬁle timestamps in general. In addition, think of application domains such as ﬁnancial brokerage, security auditing, and collaborative sensing, and it will become clear that accurate timing is important. Since time is so basic to the way people think and the effect of not having all the clocks synchronized can be so dramatic, it is ﬁtting that we begin our study of synchronization with the simple question: Is it possible to synchronize all the clocks in a distributed system? The answer is surprisingly complicated. Physical clocks Nearly all computers have a circuit for keeping track of time. Despite the widespread use of the word “clock” to refer to these devices, they are not actually clocks in the usual sense. Timer is perhaps a better word. A computer timer is usually a precisely machined quartz crystal. When kept under tension, quartz crystals oscillate at a well-deﬁned frequency that depends on the kind of crystal, how it is cut, and the amount of tension. Associated with each crystal are two registers, a counter and a holding register. Each oscillation of the crystal decrements the counter by one. When the counter gets to zero, an interrupt is generated and the counter is reloaded from the holding register. In this way, it is possible to program a timer to generate an interrupt 60 times a second, or at any other desired frequency. Each interrupt is called one clock tick. When the system is booted, it usually asks the user to enter the date and time, which is then converted to the number of ticks after some known starting downloaded by VINHNT@TNU.EDU.VN DS 3.02 300 CHAPTER 6. COORDINATION date and stored in memory. Most computers have a special battery-backed up CMOS RAM so that the date and time need not be entered on subsequent boots. At every clock tick, the interrupt service procedure adds one to the time stored in memory. In this way, the (software) clock is kept up to date. With a single computer and a single clock, it does not matter much if this clock is off by a small amount. Since all processes on the machine use the same clock, they will still be internally consistent. For example, if the ﬁle input.c has time 2151 and ﬁle input.o has time 2150, make will recompile the source ﬁle, even if the clock is off by 2 and the true times are 2153 and 2152, respectively. All that really matters are the relative times. As soon as multiple CPUs are introduced, each with its own clock, the sit- uation changes radically. Although the frequency at which a crystal oscillator runs is usually fairly stable, it is impossible to guarantee that the crystals in different computers all run at exactly the same frequency. In practice, when a system has n computers, all n crystals will run at slightly different rates, causing the (software) clocks gradually to get out of sync and give different values when read out. This difference in time values is called clock skew. As a consequence of this clock skew, programs that expect the time associated with a ﬁle, object, process, or message to be correct and independent of the machine on which it was generated (i.e., which clock it used) can fail, as we saw in the make example above. In some systems (e.g., real-time systems), the actual clock time is important. Under these circumstances, external physical clocks are needed. For reasons of efﬁciency and redundancy, multiple physical clocks are generally considered desirable, which yields two problems: (1) how do we synchronize them with real-world clocks, and (2) how do we synchronize the clocks with each other? Note 6.1 (More information: Determining real time) Before answering these questions, let us digress slightly to see how time is actually measured. It is not nearly as easy as one might think, especially when high accuracy is required. Since the invention of mechanical clocks in the 17th century, time has been measured astronomically. Every day, the sun appears to rise on the eastern horizon, then climbs to a maximum height in the sky, and ﬁnally sinks in the west. The event of the sun’s reaching its highest apparent point in the sky is called the transit of the sun. This event occurs at about noon each day. The interval between two consecutive transits of the sun is called the solar day. Since there are 24 hours in a day, each containing 3600 seconds, the solar second is deﬁned as exactly 1/86400th of a solar day. The geometry of the mean solar day calculation is shown in Figure 6.2. In the 1940s, it was established that the period of the earth’s rotation is not constant. The earth is slowing down due to tidal friction and atmospheric drag. Based on studies of growth patterns in ancient coral, geologists now believe that 300 million years ago there were about 400 days per year. The length of the year (the time for one trip around the sun) is not thought to have changed; the day has DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 301 simply become longer. In addition to this long-term trend, short-term variations in the length of the day also occur, probably caused by turbulence deep in the earth’s core of molten iron. These revelations lead astronomers to compute the length of the day by measuring a large number of days and taking the average before dividing by 86,400. The resulting quantity was called the mean solar second. Figure 6.2: Computation of the mean solar day. With the invention of the atomic clock in 1948, it became possible to measure time much more accurately, and independent of the wiggling and wobbling of the earth, by counting transitions of the cesium 133 atom. The physicists took over the job of timekeeping from the astronomers and deﬁned the second to be the time it takes the cesium 133 atom to make exactly 9,192,631,770 transitions. The choice of 9,192,631,770 was made to make the atomic second equal to the mean solar second in the year of its introduction. Currently, several laboratories around the world have cesium 133 clocks. Periodically, each laboratory tells the Bureau International de l’Heure (BIH) in Paris how many times its clock has ticked. The BIH averages these to produce International Atomic Time, which is abbreviated TAI. Thus TAI is just the mean number of ticks of the cesium 133 clocks since midnight on Jan. 1, 1958 (the beginning of time) divided by 9,192,631,770. Although TAI is highly stable and available to anyone who wants to go to the trouble of buying a cesium clock, there is a serious problem with it; 86,400 TAI seconds is now about 3 msec less than a mean solar day (because the mean solar day is getting longer all the time). Using TAI for keeping time would mean that over the course of the years, noon would get earlier and earlier, until it would eventually occur in the wee hours of the morning. People might notice this and we could have the same kind of situation as occurred in 1582 when Pope Gregory XIII decreed that 10 days be omitted from the calendar. This event caused riots in the streets because landlords demanded a full month’s rent and bankers a full month’s interest, while employers refused to pay workers for the 10 days they did not work, to mention only a few of the conﬂicts. The Protestant countries, as a matter of principle, refused to have anything to do with papal decrees and did not accept the Gregorian calendar for 170 years. downloaded by VINHNT@TNU.EDU.VN DS 3.02 302 CHAPTER 6. COORDINATION BIH solves the problem by introducing leap seconds whenever the discrepancy between TAI and solar time grows to 800 msec. The use of leap seconds is illustrated in Figure 6.3. This correction gives rise to a time system based on constant TAI seconds but which stays in phase with the apparent motion of the sun. This time system is known as UTC. Figure 6.3: TAI seconds are of constant length, unlike solar seconds. Leap seconds are introduced when necessary to keep in phase with the sun. Most electric power companies synchronize the timing of their 60-Hz or 50-Hz clocks to UTC, so when BIH announces a leap second, the power companies raise their frequency to 61 Hz or 51 Hz for 60 or 50 sec, to advance all the clocks in their distribution area. Since 1 sec is a noticeable interval for a computer, an operating system that needs to keep accurate time over a period of years must have special software to account for leap seconds as they are announced (unless they use the power line for time, which is usually too crude). The total number of leap seconds introduced into UTC so far is about 30. The basis for keeping global time is a called Universal Coordinated Time, but is abbreviated as UTC. UTC is the basis of all modern civil timekeeping and is a worldwide standard. To provide UTC to people who need precise time, some 40 shortwave radio stations around the world broadcast a short pulse at the start of each UTC second. The accuracy of these stations is about ± 1 msec, but due to random atmospheric ﬂuctuations that can affect the length of the signal path, in practice the accuracy is no better than ± 10 msec. Several earth satellites also offer a UTC service. The Geostationary Envi- ronment Operational Satellite can provide UTC accurately to 0.5 msec, and some other satellites do even better. By combining receptions from several satellites, ground time servers can be built offering an accuracy of 50 nsec. UTC receivers are commercially available and many computers are equipped with one. Clock synchronization algorithms If one machine has a UTC receiver, the goal becomes keeping all the other machines synchronized to it. If no machines have UTC receivers, each machine keeps track of its own time, and the goal is to keep all the machines together as well as possible. Many algorithms have been proposed for doing this DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 303 synchronization. Surveys are provided by Ramanathan et al. [1990], Horauer [2004], and Shin et al. [2011]. All clocks are based on some harmonic oscillator: an object that resonates at a certain frequency and from which we can subsequently derive time. Atomic clocks are based on the transitions of the cesium 133 atom, which is not only very high, but also very constant. Hardware clocks in most computers use a crystal oscillator based on quartz, which is also capable of producing a very high, stable frequency, although not as stable as that of atomic clocks. A software clock in a computer is derived from that computer’s hardware clock. In particular, the hardware clock is assumed to cause an interrupt f times per second. When this timer goes off, the interrupt handler adds 1 to a counter that keeps track of the number of ticks (interrupts) since some agreed- upon time in the past. This counter acts as a software clock C, resonating at frequency F. When the UTC time is t, denote by Cp(t) the value of the software clock on machine p. The goal of clock synchronization algorithms is to keep the deviation between the respective clocks of any two machines in a distributed system, within a speciﬁed bound, known as the precision π: ∀t, ∀p, q : |Cp(t) − Cq(t)| ≤ π Note that precision refers to the deviation of clocks only between machines that are part of a distributed system. When considering an external reference point, like UTC, we speak of accuracy, aiming to keep it bound to a value α: ∀t, ∀p : |Cp(t) − t| ≤ α The whole idea of clock synchronization is that we keep clocks precise, referred to as internal synchronization or accurate, known as external synchroniza- tion. A set of clocks that are accurate within bound α, will be precise within bound π = 2α. However, being precise does not allow us to conclude anything about the accuracy of clocks. In a perfect world, we would have Cp(t) = t for all p and all t, and thus α = π = 0. Unfortunately, hardware clocks, and thus also software clocks, are subject to clock drift: because their frequency is not perfect and affected by external sources such as temperature, clocks on different machines will gradually start showing different values for time. This is known as the clock drift rate: the difference per unit of time from a perfect reference clock. A typical quartz-based hardware clock has a clock drift rate of some 10−6 seconds per second, or approximately 31.5 seconds per year. Computer hardware clocks exist that have much lower drift rates. The speciﬁcations of a hardware clock include its maximum clock drift rate ρ. If F(t) denotes the actual oscillator frequency of the hardware clock at time t and F its ideal (constant) frequency, then a hardware clock is living up downloaded by VINHNT@TNU.EDU.VN DS 3.02 304 CHAPTER 6. COORDINATION to its speciﬁcations if ∀t : (1 − ρ) ≤ F(t) F ≤ (1 + ρ) By using hardware interrupts we are directly coupling a software clock to the hardware clock, and thus also its clock drift rate. In particular, we have that Cp(t) = 1 F ∫ t 0 F(t)dt, and thus: dCp(t) dt = F(t) F which brings us to our ultimate goal, namely keeping the software clock drift rate also bounded to ρ: ∀t : 1 − ρ ≤ dCp(t) dt ≤ 1 + ρ Slow, perfect, and fast clocks are shown in Figure 6.4. Figure 6.4: The relation between clock time and UTC when clocks tick at different rates. If two clocks are drifting from UTC in the opposite direction, at a time ∆t after they were synchronized, they may be as much as 2ρ · ∆t apart. If the system designers want to guarantee a precision π, that is, that no two clocks ever differ by more than π seconds, clocks must be resynchronized (in software) at least every π/(2ρ) seconds. The various algorithms differ in precisely how this resynchronization is done. Network Time Protocol A common approach in many protocols and originally proposed by Cristian [1989], is to let clients contact a time server. The latter can accurately provide the current time, for example, because it is equipped with a UTC receiver or an accurate clock. The problem, of course, is that when contacting the server, message delays will have outdated the reported time. The trick is to DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 305 Figure 6.5: Getting the current time from a time server. ﬁnd a good estimation for these delays. Consider the situation sketched in Figure 6.5. In this case, A will send a request to B, timestamped with value T1. B, in turn, will record the time of receipt T2 (taken from its own local clock), and returns a response timestamped with value T3, and piggybacking the previously recorded value T2. Finally, A records the time of the response’s arrival, T4. Let us assume that the propagation delays from A to B is roughly the same as B to A, meaning that δTreq = T2 − T1 ≈ T4 − T3 = δTres. In that case, A can estimate its offset relative to B as θ = T3 + (T2 − T1) + (T4 − T3) 2 − T4 = (T2 − T1) + (T3 − T4) 2 Of course, time is not allowed to run backward. If A’s clock is fast, θ < 0, meaning that A should, in principle, set its clock backward. This is not allowed as it could cause serious problems such as an object ﬁle compiled just after the clock change having a time earlier than the source which was modiﬁed just before the clock change. Such a change must be introduced gradually. One way is as follows. Suppose that the timer is set to generate 100 interrupts per second. Normally, each interrupt would add 10 msec to the time. When slowing down, the interrupt routine adds only 9 msec each time until the correction has been made. Similarly, the clock can be advanced gradually by adding 11 msec at each interrupt instead of jumping it forward all at once. In the case of the network time protocol (NTP), this protocol is set up pairwise between servers. In other words, B will also probe A for its current time. The offset θ is computed as given above, along with the estimation δ for the delay: δ = (T4 − T1) − (T3 − T2) 2 Eight pairs of (θ, δ) values are buffered, ﬁnally taking the minimal value found for δ as the best estimation for the delay between the two servers, and subsequently the associated value θ as the most reliable estimation of the offset. downloaded by VINHNT@TNU.EDU.VN DS 3.02 306 CHAPTER 6. COORDINATION Applying NTP symmetrically should, in principle, also let B adjust its clock to that of A. However, if B’s clock is known to be more accurate, then such an adjustment would be foolish. To solve this problem, NTP divides servers into strata. A server with a reference clock such as a UTC receiver or an atomic clock, is known to be a stratum-1 server (the clock itself is said to operate at stratum 0). When A contacts B, it will adjust only its time if its own stratum level is higher than that of B. Moreover, after the synchronization, A’s stratum level will become one higher than that of B. In other words, if B is a stratum-k server, then A will become a stratum-(k + 1) server if its original stratum level was already larger than k. Due to the symmetry of NTP, if A’s stratum level was lower than that of B, B will adjust itself to A. There are many important features about NTP, of which many relate to identifying and masking errors, but also security attacks. NTP was originally described in [Mills, 1992] and is known to achieve (worldwide) accuracy in the range of 1–50 msec. A detailed description of NTP can be found in [Mills, 2011]. The Berkeley algorithm In many clock synchronization algorithms the time server is passive. Other machines periodically ask it for the time. All it does is respond to their queries. In Berkeley Unix exactly the opposite approach is taken [Gusella and Zatti, 1989]. Here the time server (actually, a time daemon) is active, polling every machine from time to time to ask what time it is there. Based on the answers, it computes an average time and tells all the other machines to advance their clocks to the new time or slow their clocks down until some speciﬁed reduction has been achieved. This method is suitable for a system in which no machine has a UTC receiver. The time daemon’s time must be set manually by the operator periodically. The method is illustrated in Figure 6.6. In Figure 6.6(a) at 3:00, the time daemon tells the other machines its time and asks for theirs. In Figure 6.6(b) they respond with how far ahead or behind the time daemon they are. Armed with these numbers, the time daemon computes the average and tells each machine how to adjust its clock [see Figure 6.6(c)]. Note that for many purposes, it is sufﬁcient that all machines agree on the same time. It is not essential that this time also agrees with the real time as announced on the radio every hour. If in our example of Figure 6.6 the time daemon’s clock would never be manually calibrated, no harm is done provided none of the other nodes communicates with external computers. Everyone will just happily agree on a current time, without that value having any relation with reality. The Berkeley algorithm is thus typically an internal clock synchronization algorithm. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 307 (a) (b) (c) Figure 6.6: (a) The time daemon asks all the other machines for their clock values. (b) The machines answer. (c) The time daemon tells everyone how to adjust their clock. Clock synchronization in wireless networks An important advantage of more traditional distributed systems is that we can easily and efﬁciently deploy time servers. Moreover, most machines can contact each other, allowing for a relatively simple dissemination of infor- mation. These assumptions are no longer valid in many wireless networks, notably sensor networks. Nodes are resource constrained, and multihop routing is expensive. In addition, it is often important to optimize algorithms for energy consumption. These and other observations have led to the design of very different clock synchronization algorithms for wireless networks. In the following, we consider one speciﬁc solution. Sivrikaya and Yener [2004] provide a brief overview of other solutions. An extensive survey can be found in [Sundararaman et al., 2005]. Reference broadcast synchronization (RBS) is a clock synchronization protocol that is quite different from other proposals [Elson et al., 2002]. First, the protocol does not assume that there is a single node with an accurate account of the actual time available. Instead of aiming to provide all nodes UTC time, it aims at merely internally synchronizing the clocks, just as the Berkeley algorithm does. Second, the solutions we have discussed so far are designed to bring the sender and receiver into sync, essentially following a two-way protocol. RBS deviates from this pattern by letting only the receivers synchronize, keeping the sender out of the loop. In RBS, a sender broadcasts a reference message that will allow its receivers to adjust their clocks. A key observation is that in a sensor network the time to propagate a signal to other nodes is roughly constant, provided no multi- hop routing is assumed. Propagation time in this case is measured from the moment that a message leaves the network interface of the sender. As downloaded by VINHNT@TNU.EDU.VN DS 3.02 308 CHAPTER 6. COORDINATION a consequence, two important sources for variation in message transfer no longer play a role in estimating delays: the time spent to construct a message, and the time spent to access the network. This principle is shown in Figure 6.7. Figure 6.7: The usual critical path and the one used in RBS in determining network delays. Note that in protocols such as NTP, a timestamp is added to the message before it is passed on to the network interface. Furthermore, as wireless networks are based on a contention protocol, there is generally no saying how long it will take before a message can actually be transmitted. These factors of nondeterminism are eliminated in RBS. What remains is the delivery time at the receiver, but this time varies considerably less than the network-access time. The idea underlying RBS is simple: when a node broadcasts a reference message m, each node p simply records the time Tp,m that it received m. Note that Tp,m is read from p’s local clock. Ignoring clock skew, two nodes p and q can exchange each other’s delivery times in order to estimate their mutual, relative offset: Offset[p, q] = ∑M k=1(Tp,k − Tq,k) M where M is the total number of reference messages sent. This information is important: node p will know the value of q’s clock relative to its own value. Moreover, if it simply stores these offsets, there is no need to adjust its own clock, which saves energy. Unfortunately, clocks can drift apart. The effect is that simply computing the average offset as done above will not work: the last values sent are simply less accurate than the ﬁrst ones. Moreover, as time goes by, the offset will presumably increase. Elson et al. [2002] use a very simple algorithm to compensate for this: instead of computing an average they apply standard linear regression to compute the offset as a function: Offset[p, q](t) = αt + β DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.1. CLOCK SYNCHRONIZATION 309 The constants α and β are computed from the pairs (Tp,k, Tq,k). This new form will allow a much more accurate computation of q’s current clock value by node p, and vice versa. Note 6.2 (More information: How important is an accurate account of time?) So why is time such a big deal for distributed systems? As we shall discuss in the remainder of this chapter, reaching consensus on a global ordering of events is what we really want, and this can be achieved without any notion of global absolute time. However, as will become clear, alternative methods for distributed coordination do not come easy. And life would be much simpler if processes in a distributed system could timestamp their events with inﬁnite precision. Although inﬁnite precision is asking too much, we can come practically close. Researchers at Google were confronted with the fact that their customers would really like to make use of a globally distributed database that supported transactions. Such a database would need to serve massive numbers of clients, rendering the use of, for ex- ample, a central transaction processing monitor as we discussed in Section 1.3, infeasible. Instead, for their Spanner system, Google decided to implement a true-time service, called TrueTime [Corbett et al., 2013]. This service provides three operations: Operation Result TT.now() A time interval [Tlwb, Tupb] with Tlwb < Tupb TT.after(t) True if timestamp t has deﬁnitely passed TT.before(t) True if timestamp t has deﬁnitely not arrived The most important aspect is that Tlwb and Tupb are guaranteed bounds. Of course, if ϵ = Tupb − Tlwb is large, say 1 hour, then implementing the service is relatively easy. Impressively enough, ϵ = 6ms. To achieve this accuracy, the TrueTime service makes use of time master machines of which there are several per data center. Time-slave daemons run on every machine in a data center and query multiple time masters, including ones from other data centers, very similar to what we described for NTP. Many time masters are equipped with accurate GPS receivers, while many others are independently equipped with atomic clocks. The result is a collection of time sources with a high degree of mutual independence (which is important for reasons of fault tolerance). Using a version of an algorithm developed by Marzullo and Owicki [1983], outliers are kept out of the computations. Meanwhile, the performance of TrueTime is continuously monitored and “bad” time machines are (manually) removed to give at least very high guarantees for the accuracy of the TrueTime service. With a guaranteed accuracy of 6 milliseconds, building a transactional system becomes much easier: transactions can actually be timestamped, even by different servers, with the restriction that timestamping may need to be delayed for ϵ time units. More precisely, in order to know for sure that a transaction has committed, reading the resulting data may impose a wait for ϵ units. This is achieved by pessimistically assigning a timestamp to a transaction that writes data to the downloaded by VINHNT@TNU.EDU.VN DS 3.02 310 CHAPTER 6. COORDINATION global database and making sure that clients never see any changes before the assigned timestamp (which is relatively easy to implement). There are many details to this approach, which can be found in [Corbett et al., 2013]. As we are still dealing with a time interval, taking more traditional ordering mechanisms into account it is possible to improve results, as explained by Demirbas and Kulkarni [2013]. 6.2 Logical clocks Clock synchronization is naturally related to time, although it may not be necessary to have an accurate account of the real time: it may be sufﬁcient that every node in a distributed systems agrees on a current time. We can go one step further. For running make it is adequate that two nodes agree that input.o is outdated by a new version of input.c, for example. In this case, keeping track of each other’s events (such as a producing a new version of input.c) is what matters. For these algorithms, it is conventional to speak of the clocks as logical clocks. In a seminal paper, Lamport [1978] showed that although clock synchro- nization is possible, it need not be absolute. If two processes do not interact, it is not necessary that their clocks be synchronized because the lack of syn- chronization would not be observable and thus could not cause problems. Furthermore, he pointed out that what usually matters is not that all processes agree on exactly what time it is, but rather that they agree on the order in which events occur. In the make example, what counts is whether input.c is older or newer than input.o, not their respective absolute creation times. Lamport’s logical clocks To synchronize logical clocks, Lamport deﬁned a relation called happens- before. The expression a → b is read “event a happens before event b” and means that all processes agree that ﬁrst event a occurs, then afterward, event b occurs. The happens-before relation can be observed directly in two situations: 1. If a and b are events in the same process, and a occurs before b, then a → b is true. 2. If a is the event of a message being sent by one process, and b is the event of the message being received by another process, then a → b is also true. A message cannot be received before it is sent, or even at the same time it is sent, since it takes a ﬁnite, nonzero amount of time to arrive. Happens-before is a transitive relation, so if a → b and b → c, then a → c. If two events, x and y, happen in different processes that do not exchange DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.2. LOGICAL CLOCKS 311 messages (not even indirectly via third parties), then x → y is not true, but neither is y → x. These events are said to be concurrent, which simply means that nothing can be said (or need be said) about when the events happened or which event happened ﬁrst. What we need is a way of measuring a notion of time such that for every event, a, we can assign it a time value C(a) on which all processes agree. These time values must have the property that if a → b, then C(a) < C(b). To rephrase the conditions we stated earlier, if a and b are two events within the same process and a occurs before b, then C(a) < C(b). Similarly, if a is the sending of a message by one process and b is the reception of that message by another process, then C(a) and C(b) must be assigned in such a way that everyone agrees on the values of C(a) and C(b) with C(a) < C(b). In addition, the clock time, C, must always go forward (increasing), never backward (decreasing). Corrections to time can be made by adding a positive value, never by subtracting one. Now let us look at the algorithm Lamport proposed for assigning times to events. Consider the three processes depicted in Figure 6.8. The processes run on different machines, each with its own clock. For the sake of argument, we assume that a clock is implemented as a software counter: the counter is incremented by a speciﬁc value every T time units. However, the value by which a clock is incremented differs per process. The clock in process P1 is incremented by 6 units, 8 units in process P2, and 10 units in process P3, respectively. (Below, we explain that Lamport clocks are, in fact, event counters, which explains why their value may differ between processes.) (a) (b) Figure 6.8: (a) Three processes, each with its own (logical) clock. The clocks run at different rates. (b) Lamport’s algorithm corrects their values. At time 6, process P1 sends message m1 to process P2. How long this message takes to arrive depends on whose clock you believe. In any event, the clock in process P2 reads 16 when it arrives. If the message carries the downloaded by VINHNT@TNU.EDU.VN DS 3.02 312 CHAPTER 6. COORDINATION starting time, 6, in it, process P2 will conclude that it took 10 ticks to make the journey. This value is certainly possible. According to this reasoning, message m2 from P2 to P3 takes 16 ticks, again a plausible value. Now consider message m3. It leaves process P3 at 60 and arrives at P2 at 56. Similarly, message m4 from P2 to P1 leaves at 64 and arrives at 54. These values are clearly impossible. It is this situation that must be prevented. Lamport’s solution follows directly from the happens-before relation. Since m3 left at 60, it must arrive at 61 or later. Therefore, each message carries the sending time according to the sender’s clock. When a message arrives and the receiver’s clock shows a value prior to the time the message was sent, the receiver fast forwards its clock to be one more than the sending time. In Figure 6.8, we see that m3 now arrives at 61. Similarly, m4 arrives at 70. Let us formulate this procedure more precisely. At this point, it is impor- tant to distinguish three different layers of software, as we already encountered in Chapter 1: the network, a middleware layer, and an application layer, as shown in Figure 6.9. What follows is typically part of the middleware layer. Figure 6.9: The positioning of Lamport’s logical clocks in distributed systems. To implement Lamport’s logical clocks, each process Pi maintains a local counter Ci. These counters are updated according to the following steps [Ray- nal and Singhal, 1996]: 1. Before executing an event (i.e., sending a message over the network, delivering a message to an application, or some other internal event), Pi increments Ci: Ci ← Ci + 1. 2. When process Pi sends a message m to process Pj, it sets m’s timestamp ts(m) equal to Ci after having executed the previous step. 3. Upon the receipt of a message m, process Pj adjusts its own local counter as Cj ← max{Cj, ts(m)} after which it then executes the ﬁrst step and delivers the message to the application. In some situations, an additional requirement is desirable: no two events ever occur at exactly the same time. To achieve this goal, we also use the unique DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.2. LOGICAL CLOCKS 313 process identiﬁer to break ties and use tuples instead of only the counter’s values. For example, an event at time 40 at process Pi will be timestamped as ⟨40, i⟩. If we also have an event ⟨40, j⟩ and i < j, then ⟨40, i⟩ < ⟨40, j⟩. Note that by assigning the event time C(a) ← Ci(a) if a happened at process Pi at time Ci(a), we have a distributed implementation of the global time value we were initially seeking for; we have thus constructed a logical clock. Example: Total-ordered multicasting As an application of Lamport’s logical clocks, consider the situation in which a database has been replicated across several sites. For example, to improve query performance, a bank may place copies of an account database in two different cities, say New York and San Francisco. A query is always forwarded to the nearest copy. The price for a fast response to a query is partly paid in higher update costs, because each update operation must be carried out at each replica. In fact, there is a more stringent requirement with respect to updates. Assume a customer in San Francisco wants to add $100 to his account, which currently contains $1,000. At the same time, a bank employee in New York initiates an update by which the customer’s account is to be increased with 1 percent interest. Both updates should be carried out at both copies of the database. However, due to communication delays in the underlying network, the updates may arrive in the order as shown in Figure 6.10. Figure 6.10: Updating a replicated database and leaving it in an inconsistent state. The customer’s update operation is performed in San Francisco before the interest update. In contrast, the copy of the account in the New York replica is ﬁrst updated with the 1 percent interest, and after that with the $100 deposit. Consequently, the San Francisco database will record a total amount of $1,111, whereas the New York database records $1,110. The problem that we are faced with is that the two update operations should have been performed in the same order at each copy. Although it downloaded by VINHNT@TNU.EDU.VN DS 3.02 314 CHAPTER 6. COORDINATION makes a difference whether the deposit is processed before the interest update or the other way around, which order is followed is not important from a consistency point of view. The important issue is that both copies should be exactly the same. In general, situations such as these require a total-ordered multicast, that is, a multicast operation by which all messages are delivered in the same order to each receiver. Lamport’s logical clocks can be used to implement total-ordered multicasts in a completely distributed fashion. Consider a group of processes multicasting messages to each other. Each message is always timestamped with the current (logical) time of its sender. When a message is multicast, it is conceptually also sent to the sender. In addition, we assume that messages from the same sender are received in the order they were sent, and that no messages are lost. When a process receives a message, it is put into a local queue, ordered according to its timestamp. The receiver multicasts an acknowledgment to the other processes. Note that if we follow Lamport’s algorithm for adjusting local clocks, the timestamp of the received message is lower than the timestamp of the acknowledgment. The interesting aspect of this approach is that all processes will eventually have the same copy of the local queue (provided no messages are removed). A process can deliver a queued message to the application it is running only when that message is at the head of the queue and has been acknowl- edged by each other process. At that point, the message is removed from the queue and handed over to the application; the associated acknowledgments can simply be removed. Because each process has the same copy of the queue, all messages are delivered in the same order everywhere. In other words, we have established total-ordered multicasting. We leave it as an exercise to the reader to ﬁgure out that it is not strictly necessary that each multicast message has been explicitly acknowledged. It is sufﬁcient that a process reacts to an incoming message either by returning an acknowledgment or sending its own multicast message. Total-ordered multicasting is an important vehicle for replicated services where the replicas are kept consistent by letting them execute the same operations in the same order everywhere. As the replicas essentially follow the same transitions in the same ﬁnite state machine, it is also known as state machine replication [Schneider, 1990]. Note 6.3 (Advanced: Using Lamport clocks to achieve mutual exclusion) To further illustrate the usage of Lamport’s clocks, let us see how we can use the previous algorithm for total-ordered multicasting to establish access to what is commonly known as a critical section: a section of code that can be executed by at most one process at a time. This algorithm is very similar to the one for multicasting, as essentially all processes need to agree on the order by which processes are allowed to enter their critical section. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.2. LOGICAL CLOCKS 315 Figure 6.11(a) shows the code that each process executes when requesting, releasing, or allowing access to the critical section (again, leaving out details). Each process maintains a request queue as well as a logical clock. To enter the critical section, a call to requestToEnter is made, which results in inserting an ENTER message with timestamp (clock,procID) into the local queue and sending that message to the other processes. The operation cleanupQ essentially sorts the queue. We return to it shortly. 1 class Process: 2 def __init__(self, chan): 3 self.queue = [] # The request queue 4 self.clock = 0 # The current logical clock 5 6 def requestToEnter(self): 7 self.clock = self.clock + 1 # Increment clock value 8 self.queue.append((self.clock, self.procID, ENTER)) # Append request to q 9 self.cleanupQ() # Sort the queue 10 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,ENTER)) # Send request 11 12 def allowToEnter(self, requester): 13 self.clock = self.clock + 1 # Increment clock value 14 self.chan.sendTo([requester], (self.clock,self.procID,ALLOW)) # Permit other 15 16 def release(self): 17 tmp = [r for r in self.queue[1:] if r[2] == ENTER] # Remove all ALLOWs 18 self.queue = tmp # and copy to new queue 19 self.clock = self.clock + 1 # Increment clock value 20 self.chan.sendTo(self.otherProcs, (self.clock,self.procID,RELEASE)) # Release 21 22 def allowedToEnter(self): 23 commProcs = set([req[1] for req in self.queue[1:]]) # See who has sent a message 24 return (self.queue[0][1]==self.procID and len(self.otherProcs)==len(commProcs)) Figure 6.11: (a) Using Lamport’s logical clocks for mutual exclusion. When a process P receives an ENTER message from process Q, it can simply allow Q to enter its critical section, even if P wants to do so as well. In that case, P’s request will have a lower logical timestamp than the ALLOW message sent by P to Q, meaning that P’s request will have been inserted into Q’s queue before P’s ALLOW message. Finally, when a process leaves its critical section, it calls release. It cleans up its local queue by removing all received ALLOW messages, leaving only the ENTER requests from other processes. It then multicasts a RELEASE message. In order to actually enter a critical section, a process will have to repeatedly call allowedToEnter and when returned False, will have to block on a next incoming message. The operation allowedToEnter does what is to be expected: it checks if the calling process’s ENTER message is at the head of the queue, and sees if all other processes have sent a message as well. The latter is encoded through the set commProcs, which contains the procIDs of all processes having sent a message by inspecting all messages in the local queue from the second position and onwards. downloaded by VINHNT@TNU.EDU.VN DS 3.02 316 CHAPTER 6. COORDINATION 1 def receive(self): 2 msg = self.chan.recvFrom(self.otherProcs)[1] # Pick up any message 3 self.clock = max(self.clock, msg[0]) # Adjust clock value... 4 self.clock = self.clock + 1 # ...and increment 5 if msg[2] == ENTER: 6 self.queue.append(msg) # Append an ENTER request 7 self.allowToEnter(msg[1]) # and unconditionally allow 8 elif msg[2] == ALLOW: 9 self.queue.append(msg) # Append an ALLOW 10 elif msg[2] == RELEASE: 11 del(self.queue[0]) # Just remove first message 12 self.cleanupQ() # And sort and cleanup Figure 6.11: (b) Using Lamport’s logical clocks for mutual exclusion: handling incoming requests. What to do when a message is received is shown in Figure 6.11(b). First, the local clock is adjusted according to the rules for Lamport’s logical clocks explained above. When receiving an ENTER or ALLOW message, that message is simply inserted into the queue. An entry request is always acknowledged, as we just explained. When a RELEASE message is received, the original ENTER request is removed. Note that this request is at the head of the queue. After that, the queue is cleaned up again. At this point, note that if we would clean up the queue by only sorting it, we may get into trouble. Suppose that processes P and Q want to enter their respective critical sections at roughly the same time, but that P is allowed to go ﬁrst based on logical-clock values. P may ﬁnd Q’s request in its queue, along with ENTER or ALLOW messages from other processes. If its own request is at the head of its queue, P will proceed and enter its critical section. However, Q will also send an ALLOW message to P as well, in addition to its original ENTER message. That ALLOW message may arrive after P had already entered its critical section, but before ENTER messages from other processes. When Q eventually enters, and leaves its critical section, Q’s RELEASE message would result in removing Q’s original ENTER message, but not the ALLOW message it had previously sent to P. By now, that message is at the head of P’s queue, effectively blocking the entrance to the critical section of other processes in P’s queue. Cleaning up the queue thus also involves removing old ALLOW messages. Vector clocks Lamport’s logical clocks lead to a situation where all events in a distributed system are totally ordered with the property that if event a happened before event b, then a will also be positioned in that ordering before b, that is, C(a) < C(b). However, with Lamport clocks, nothing can be said about the relationship between two events a and b by merely comparing their time values C(a) and C(b), respectively. In other words, if C(a) < C(b), then this does not DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.2. LOGICAL CLOCKS 317 necessarily imply that a indeed happened before b. Something more is needed for that. To explain, consider the messages as sent by the three processes shown in Figure 6.12. Denote by Tsnd(mi) the logical time at which message mi was sent, and likewise, by Trcv(mi) the time of its receipt. By construction, we know that for each message Tsnd(mi) < Trcv(mi). But what can we conclude in general from Trcv(mi) < Tsnd(mj) for different messages mi and mj? Figure 6.12: Concurrent message transmission using logical clocks. In the case for which mi = m1 and mj = m3, we know that these values correspond to events that took place at process P2, meaning that m3 was indeed sent after the receipt of message m1. This may indicate that the sending of message m3 depended on what was received through message m1. At the same time, we also know that Trcv(m1) < Tsnd(m2). However, as far as we can tell from Figure 6.12, the sending of m2 has nothing to do with the receipt of m1. The problem is that Lamport clocks do not capture causality. In practice, causality is captured by means of vector clocks. To better understand where these come from, we follow the explanation as given by Baquero and Preguica [2016]. In fact, tracking causality is simple if we assign each event a unique name such as the combination of a process ID and a locally incrementing counter: pk is the kth event that happened at process P. The problem then boils down to keeping track of causal histories. For example, if two local events happened at process P, then the causal history H(p2) of event p2 is {p1, p2}. Now assume that process P sends a message to process Q (which is an event at P and thus recorded as pk from some k), and that at the time of arrival (and event for Q), the most recent causal history of Q was {q1}. To track causality, P also sends its most recent causal history (assume it was {p1, p2}, extended with p3 expressing the sending of the message). Upon arrival, Q records the event (q2), and merges the two causal histories into a new one: downloaded by VINHNT@TNU.EDU.VN DS 3.02 318 CHAPTER 6. COORDINATION {p1, p2, p3, q1, q2}. Checking whether an event p causally precedes an event q can be done by checking whether H(p) ⊂ H(q) (i.e., it should be a proper subset). In fact, with our notation, it even sufﬁces to check whether p ∈ H(q), assuming that q is always the last local event in H(q). The problem with causal histories, is that their representation is not very efﬁcient. However, there is no need to keep track of all successive events from the same process: the last one will do. If we subsequently assign an index to each process, we can represent a causal history as a vector, in which the jth entry represents the number of events that happened at process Pj. Causality can then be captured by means of vector clocks, which are constructed by letting each process Pi maintain a vector VCi with the following two properties: 1. VCi[i] is the number of events that have occurred so far at Pi. In other words, VCi[i] is the local logical clock at process Pi. 2. If VCi[j] = k then Pi knows that k events have occurred at Pj. It is thus Pi’s knowledge of the local time at Pj. The ﬁrst property is maintained by incrementing VCi[i] at the occurrence of each new event that happens at process Pi. The second property is maintained by piggybacking vectors along with messages that are sent. In particular, the following steps are performed: 1. Before executing an event (i.e., sending a message over the network, delivering a message to an application, or some other internal event), Pi executes VCi[i] ← VCi[i] + 1. This is equivalent to recording a new event that happened at Pi. 2. When process Pi sends a message m to Pj, it sets m’s (vector) timestamp ts(m) equal to VCi after having executed the previous step (i.e., it also records the sending of the message as an event that takes place at Pi). 3. Upon the receipt of a message m, process Pj adjusts its own vector by setting VCj[k] ← max{VCj[k], ts(m)[k]} for each k (which is equivalent to merging causal histories), after which it executes the ﬁrst step (record- ing the receipt of the message) and then delivers the message to the application. Note that if an event a has timestamp ts(a), then ts(a)[i] − 1 denotes the number of events processed at Pi that causally precede a. As a consequence, when Pj receives a message m from Pi with timestamp ts(m), it knows about the number of events that have occurred at Pi that causally preceded the sending of m. More important, however, is that Pj is also told how many events at other processes have taken place, known to Pi, before Pi sent message m. In other words, timestamp ts(m) tells the receiver how many events in other processes have preceded the sending of m, and on which m may causally depend. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.2. LOGICAL CLOCKS 319 To see what this means, consider Figure 6.13 which shows three processes. In Figure 6.13(a), P2 sends a message m1 at logical time VC2 = (0, 1, 0) to process P1. Message m1 thus receives timestamp ts(m1) = (0, 1, 0). Upon its receipt, P1 adjusts its logical time to VC1 ← (1, 1, 0) and delivers it. Mes- sage m2 is sent by P1 to P3 with timestamp ts(m2) = (2, 1, 0). Before P1 sends another message, m3, an event happens at P1, eventually leading to timestamping m3 with value (4, 1, 0). After receiving m3, process P2 sends message m4 to P3, with timestamp ts(m4) = (4, 3, 0). (a) (b) Figure 6.13: Capturing potential causality when exchanging messages. Now consider the situation shown in Figure 6.13(b). Here, we have delayed sending message m2 until after message m3 has been sent, and after the event had taken place. It is not difﬁcult to see that ts(m2) = (4, 1, 0), while ts(m4) = (2, 3, 0). Compared to Figure 6.13(a), we have the following situation: Situation ts(m2) ts(m4) ts(m2) ts(m2) Conclusion < > ts(m4) ts(m4) Figure 6.13(a) (2, 1, 0) (4, 3, 0) Yes No m2 may causally precede m4 Figure 6.13(b) (4, 1, 0) (2, 3, 0) No No m2 and m4 may conﬂict We use the notation ts(a) < ts(b) if and only if for all k, ts(a)[k] ≤ ts(b)[k] and there is at least one index k′ for which ts(a)[k′] < ts(b)[k′]. Thus, by using vector clocks, process P3 can detect whether m4 may be causally dependent on m2, or whether there may be a potential conﬂict. Note, by the way, that downloaded by VINHNT@TNU.EDU.VN DS 3.02 320 CHAPTER 6. COORDINATION without knowing the actual information contained in messages, it is not possible to state with certainty that there is indeed a causal relationship, or perhaps a conﬂict. Note 6.4 (Advanced: Enforcing causal communication) Using vector clocks, it is now possible to ensure that a message is delivered only if all messages that may have causally precede it have been received as well. To enable such a scheme, we will assume that messages are multicast within a group of processes. Note that this causal-ordered multicasting is weaker than total-ordered multicasting. Speciﬁcally, if two messages are not in any way related to each other, we do not care in which order they are delivered to applications. They may even be delivered in different order at different locations. For enforcing causal message delivery, we assume that clocks are adjusted only when sending and delivering messages (note, again, that messages are not adjusted when they are received by a process, but only when they are delivered to an application). In particular, upon sending a message, process Pi will only increment VCi[i] by 1. When it delivers a message m with timestamp ts(m), it only adjusts VCi[k] to max{VCi[k], ts(m)[k]} for each k. Now suppose that Pj receives a message m from Pi with (vector) timestamp ts(m). The delivery of the message to the application layer will then be delayed until the following two conditions are met: 1. ts(m)[i] = VCj[i] + 1 2. ts(m)[k] ≤ VCj[k] for all k ̸= i The ﬁrst condition states that m is the next message that Pj was expecting from process Pi. The second condition states that Pj has delivered all the messages that have been delivered by Pi when it sent message m. Note that there is no need for process Pj to delay the delivery of its own messages. Figure 6.14: Enforcing causal communication. As an example, consider three processes P1, P2, and P3 as shown in Figure 6.14. At local time (1, 0, 0), P1 sends message m to the other two processes. Note that ts(m) = (1, 0, 0). Its receipt and subsequent delivery by P2, will bring the logical clock at P2 to (1, 0, 0), effectively indicating that it has received one message from P1, has itself sent no message so far, and has not yet received a message from P3. P2 then decides to send m∗, at updated time (1, 1, 0), which arrives at P3 sooner than m. When comparing the timestamp of m with its current time, which is (0, 0, 0), P3 concludes that it is still missing a message from P1 which P2 apparently had DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.3. MUTUAL EXCLUSION 321 delivered before sending m∗. P3 therefore decides to postpone the delivery of m∗ (and will also not adjust its local, logical clock). Later, after m has been received and delivered by P3, which brings its local clock to (1, 0, 0), P3 can deliver message m∗ and also update its clock. A note on ordered message delivery. Some middleware systems, notably ISIS and its successor Horus [Birman and van Renesse, 1994], provide support for total-ordered and causal-ordered (reliable) multicasting. There has been some controversy whether such support should be provided as part of the message- communication layer, or whether applications should handle ordering (see, e.g., Cheriton and Skeen [1993]; Birman [1994]). Matters have not been settled, but more important is that the arguments still hold today. There are two main problems with letting the middleware deal with message ordering. First, because the middleware cannot tell what a message actually contains, only potential causality is captured. For example, two messages from the same sender that are completely independent will always be marked as causally related by the middleware layer. This approach is overly restrictive and may lead to efﬁciency problems. A second problem is that not all causality may be captured. Consider an electronic bulletin board. Suppose Alice posts an article. If she then phones Bob telling about what she just wrote, Bob may post another article as a reaction without having seen Alice’s posting on the board. In other words, there is a causality between Bob’s posting and that of Alice due to external communication. This causality is not captured by the bulletin board system. In essence, ordering issues, like many other application-speciﬁc communi- cation issues, can be adequately solved by looking at the application for which communication is taking place. This is also known as the end-to-end argument in systems design [Saltzer et al., 1984]. A drawback of having only application-level solutions is that a developer is forced to concentrate on issues that do not immedi- ately relate to the core functionality of the application. For example, ordering may not be the most important problem when developing a messaging system such as an electronic bulletin board. In that case, having an underlying communication layer handle ordering may turn out to be convenient. We will come across the end-to-end argument a number of times. 6.3 Mutual exclusion Fundamental to distributed systems is the concurrency and collaboration among multiple processes. In many cases, this also means that processes will need to simultaneously access the same resources. To prevent that such concurrent accesses corrupt the resource, or make it inconsistent, solutions are needed to grant mutual exclusive access by processes. In this section, we take a look at some important and representative distributed algorithms that have been proposed. Surveys of distributed algorithms for mutual exclusion are downloaded by VINHNT@TNU.EDU.VN DS 3.02 322 CHAPTER 6. COORDINATION provided by Saxena and Rai [2003] and Velazquez [1993]. Various algorithms are also presented in [Kshemkalyani and Singhal, 2008]. Overview Distributed mutual exclusion algorithms can be classiﬁed into two different categories. In token-based solutions mutual exclusion is achieved by passing a special message between the processes, known as a token. There is only one token available and who ever has that token is allowed to access the shared resource. When ﬁnished, the token is passed on to a next process. If a process having the token is not interested in accessing the resource, it passes it on. Token-based solutions have a few important properties. First, depending on how the processes are organized, they can fairly easily ensure that every process will get a chance at accessing the resource. In other words, they avoid starvation. Second, deadlocks by which several processes are indeﬁnitely waiting for each other to proceed, can easily be avoided, contributing to their simplicity. The main drawback of token-based solutions is a rather serious one: when the token is lost (e.g., because the process holding it crashed), an intricate distributed procedure needs to be started to ensure that a new token is created, but above all, that it is also the only token. As an alternative, many distributed mutual exclusion algorithms follow a permission-based approach. In this case, a process wanting to access the resource ﬁrst requires the permission from other processes. There are many different ways toward granting such a permission and in the sections that follow we will consider a few of them. A centralized algorithm A straightforward way to achieve mutual exclusion in a distributed system is to simulate how it is done in a one-processor system. One process is elected as the coordinator. Whenever a process wants to access a shared resource, it sends a request message to the coordinator stating which resource it wants to access and asking for permission. If no other process is currently accessing that resource, the coordinator sends back a reply granting permission, as shown in Figure 6.15(a). When the reply arrives, the requester can go ahead. Now suppose that another process, P2 in Figure 6.15(b) asks for permission to access the resource. The coordinator knows that a different process is already at the resource, so it cannot grant permission. The exact method used to deny permission is system dependent. In Figure 6.15(b) the coordinator just refrains from replying, thus blocking process P2, which is waiting for a reply. Alternatively, it could send a reply saying “permission denied.” Either way, it queues the request from P2 for the time being and waits for more messages. When process P1 is ﬁnished with the resource, it sends a message to the coordinator releasing its exclusive access, as shown in Figure 6.15(c). The DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.3. MUTUAL EXCLUSION 323 (a) (b) (c) Figure 6.15: (a) Process P1 asks for permission to access a shared resource. Per- mission is granted. (b) Process P2 asks permission to access the same resource, but receives no reply. (c) When P1 releases the resource, the coordinator replies to P2. coordinator takes the ﬁrst item off the queue of deferred requests and sends that process a grant message. If the process was still blocked (i.e., this is the ﬁrst message to it), it unblocks and accesses the resource. If an explicit message has already been sent denying permission, the process will have to poll for incoming trafﬁc or block later. Either way, when it sees the grant, it can go ahead as well. It is easy to see that the algorithm guarantees mutual exclusion: the coordinator lets only one process at a time access the resource. It is also fair, since requests are granted in the order in which they are received. No process ever waits forever (no starvation). The scheme is easy to implement, too, and requires only three messages per use of resource (request, grant, release). Its simplicity makes it an attractive solution for many practical situations. The centralized approach also has shortcomings. The coordinator is a single point of failure, so if it crashes, the entire system may go down. If processes normally block after making a request, they cannot distinguish a dead coordinator from “permission denied” since in both cases no message comes back. In addition, in a large system, a single coordinator can become a performance bottleneck. Nevertheless, the beneﬁts coming from its simplicity outweigh in many cases the potential drawbacks. Moreover, distributed solutions are not necessarily better, as we illustrate in Section 6.3. A distributed algorithm Using Lamport’s logical clocks, and inspired by Lamport’s original solution for distributed mutual exclusion (which we discussed in ) Note 6.3, Ricart and Agrawala [1981] provided the following algorithm. Their solution requires a total ordering of all events in the system. That is, for any pair of events, such as messages, it must be unambiguous which one actually happened ﬁrst. The algorithm works as follows. When a process wants to access a shared downloaded by VINHNT@TNU.EDU.VN DS 3.02 324 CHAPTER 6. COORDINATION resource, it builds a message containing the name of the resource, its process number, and the current (logical) time. It then sends the message to all other processes, conceptually including itself. The sending of messages is assumed to be reliable; that is, no message is lost. When a process receives a request message from another process, the action it takes depends on its own state with respect to the resource named in the message. Three different cases have to be clearly distinguished: • If the receiver is not accessing the resource and does not want to access it, it sends back an OK message to the sender. • If the receiver already has access to the resource, it simply does not reply. Instead, it queues the request. • If the receiver wants to access the resource as well but has not yet done so, it compares the timestamp of the incoming message with the one contained in the message that it has sent everyone. The lowest one wins. If the incoming message has a lower timestamp, the receiver sends back an OK message. If its own message has a lower timestamp, the receiver queues the incoming request and sends nothing. After sending out requests asking permission, a process sits back and waits until everyone else has given permission. As soon as all the permissions are in, it may go ahead. When it is ﬁnished, it sends OK messages to all processes in its queue and deletes them all from the queue. If there is no conﬂict, it clearly works. However, suppose that two processes try to simultaneously access the resource, as shown in Figure 6.16(a). (a) (b) (c) Figure 6.16: (a) Two processes want to access a shared resource at the same moment. (b) P0 has the lowest timestamp, so it wins. (c) When process P0 is done, it sends an OK also, so P2 can now go ahead. Process P0 sends everyone a request with timestamp 8, while at the same time, process P2 sends everyone a request with timestamp 12. P1 is not interested in the resource, so it sends OK to both senders. Processes P0 and P2 both see the conﬂict and compare timestamps. P2 sees that it has lost, DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.3. MUTUAL EXCLUSION 325 so it grants permission to P0 by sending OK. Process P0 now queues the request from P2 for later processing and accesses the resource, as shown in Figure 6.16(b). When it is ﬁnished, it removes the request from P2 from its queue and sends an OK message to P2, allowing the latter to go ahead, as shown in Figure 6.16(c). The algorithm works because in the case of a conﬂict, the lowest timestamp wins and everyone agrees on the ordering of the timestamps. With this algorithm, mutual exclusion is guaranteed without deadlock or starvation. If the total number of processes is N, then the number of messages that a process needs to send and receive before it can enter its critical section is 2 · (N − 1): N − 1 request messages to all other processes, and subsequently N − 1 OK messages, one from each other process. Unfortunately, this algorithm has N points of failure. If any process crashes, it will fail to respond to requests. This silence will be interpreted (incorrectly) as denial of permission, thus blocking all subsequent attempts by all processes to enter any of their respective critical regions. The algorithm can be patched up as follows. When a request comes in, the receiver always sends a reply, either granting or denying permission. Whenever either a request or a reply is lost, the sender times out and keeps trying until either a reply comes back or the sender concludes that the destination is dead. After a request is denied, the sender should block waiting for a subsequent OK message. Another problem with this algorithm is that either a multicast commu- nication primitive must be used, or each process must maintain the group membership list itself, including processes entering the group, leaving the group, and crashing. The method works best with small groups of processes that never change their group memberships. Finally, note that all processes are involved in all decisions concerning accessing the shared resource, which may impose a burden on processes running on resource-constrained machines. Various minor improvements are possible to this algorithm. For example, getting permission from everyone is overkill. All that is needed is a method to prevent two processes from accessing the resource at the same time. The algo- rithm can be modiﬁed to grant permission when it has collected permission from a simple majority of the other processes, rather than from all of them. A token-ring algorithm A completely different approach to deterministically achieving mutual ex- clusion in a distributed system is illustrated in Figure 6.17. In software, we construct an overlay network in the form of a logical ring in which each process is assigned a position in the ring. All that matters is that each process knows who is next in line after itself. When the ring is initialized, process P0 is given a token. The token circulates around the ring. Assuming there are N processes, the token is passed from process Pk to process P(k+1) mod N in point-to-point messages. downloaded by VINHNT@TNU.EDU.VN DS 3.02 326 CHAPTER 6. COORDINATION Figure 6.17: An overlay network constructed as a logical ring with a token circulating between its members. When a process acquires the token from its neighbor, it checks to see if it needs to access the shared resource. If so, the process goes ahead, does all the work it needs to, and releases the resources. After it has ﬁnished, it passes the token along the ring. It is not permitted to immediately enter the resource again using the same token. If a process is handed the token by its neighbor and is not interested in the resource, it just passes the token along. As a consequence, when no processes need the resource, the token just circulates around the ring. The correctness of this algorithm is easy to see. Only one process has the token at any instant, so only one process can actually get to the resource. Since the token circulates among the processes in a well-deﬁned order, starvation cannot occur. Once a process decides it wants to have access to the resource, at worst it will have to wait for every other process to use the resource. This algorithm has its own problems. If the token is ever lost, for example, because its holder crashes or due to a lost message containing the token, it must be regenerated. In fact, detecting that it is lost may be difﬁcult, since the amount of time between successive appearances of the token on the network is unbounded. The fact that the token has not been spotted for an hour does not mean that it has been lost; somebody may still be using it. The algorithm also runs into trouble if a process crashes, but recovery is relatively easy. If we require a process receiving the token to acknowledge receipt, a dead process will be detected when its neighbor tries to give it the token and fails. At that point the dead process can be removed from the group, and the token holder can throw the token over the head of the dead process to the next member down the line, or the one after that, if necessary. Of course, doing so requires that everyone maintains the current ring conﬁguration. A decentralized algorithm Let us take a look at fully decentralized solution. Lin et al. [2004] propose to use a voting algorithm. Each resource is assumed to be replicated N times. Every replica has its own coordinator for controlling the access by concurrent processes. However, whenever a process wants to access the resource, it will simply need to get a majority vote from m > N/2 coordinators. We assume that DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.3. MUTUAL EXCLUSION 327 when a coordinator does not give permission to access a resource (which it will do when it had granted permission to another process), it will tell the requester. The assumption is that when a coordinator crashes, it recovers quickly but will have forgotten any vote it gave before it crashed. Another way of viewing this is that a coordinator resets itself at arbitrary moments. The risk that we are taking is that a reset will make the coordinator forget that it had previously granted permission to some process to access the resource. As a consequence, it may incorrectly grant this permission again to another process after its recovery. Let p = ∆t/T be the probability that a coordinator resets during a time interval ∆t, while having a lifetime of T. The probability P[k] that k out of m coordinators reset during the same interval is then P[k] = (m k )pk(1 − p)m−k If f coordinators reset, then the correctness of the voting mechanism will be violated when we have only a minority of nonfaulty coordinators, that is, when m − f ≤ N/2, or, in other words, when f ≥ m − N/2. The probability that such a violation occurs is then ∑m k=m−N/2 P[k]. To give an impression of what this could mean, Figure 6.18 shows the probability of violating correctness for different values of N, m, and p. Note that we compute p by considering the number of seconds per hour that a coordinator resets, and also taking this value to be the average time needed to access a resource. Our values for p are considered to be (very) conservative. The conclusion is that, in general, the probability of violating correctness can be so low that it can be neglected in comparison to other types of failure. N m p Violation 8 5 3 sec/hour < 10−15 8 6 3 sec/hour < 10−18 16 9 3 sec/hour < 10−27 16 12 3 sec/hour < 10−36 32 17 3 sec/hour < 10−52 32 24 3 sec/hour < 10−73 N m p Violation 8 5 30 sec/hour < 10−10 8 6 30 sec/hour < 10−11 16 9 30 sec/hour < 10−18 16 12 30 sec/hour < 10−24 32 17 30 sec/hour < 10−35 32 24 30 sec/hour < 10−49 Figure 6.18: Violation probabilities for various parameter values of decentral- ized mutual exclusion. To implement this scheme, we can use a system in which a resource is replicated N times. Assume that the resource is known under its unique name rname. We can then assume that the i-th replica is named rnamei which downloaded by VINHNT@TNU.EDU.VN DS 3.02 328 CHAPTER 6. COORDINATION is then used to compute a unique key using a known hash function. As a consequence, every process can generate the N keys given a resource’s name, and subsequently look up each node responsible for a replica (and controlling access to that replica) using some commonly used naming system. If permission to access the resource is denied (i.e., a process gets less than m votes), it is assumed that it will back off for some randomly chosen time, and make a next attempt later. The problem with this scheme is that if many nodes want to access the same resource, it turns out that the utilization rapidly drops. In that case, there are so many nodes competing to get access that eventually no one is able to get enough votes leaving the resource unused. A solution to solve this problem can be found in [Lin et al., 2004]. Note 6.5 (More information: A comparison of the mutual-exclusion algorithms) Messages per Delay before entry Algorithm entry/exit (in message times) Centralized 3 2 Distributed 2 · (N − 1) 2 · (N − 1) Token ring 1, . . . , ∞ 0, . . . , N − 1 Decentralized 2 · m · k + m, k = 1, 2, . . . 2 · m · k Figure 6.19: A comparison of four mutual exclusion algorithms. A brief comparison of the mutual exclusion algorithms we have looked at is instructive. In Figure 6.19 we have listed the algorithms and two performance properties: the number of messages required for a process to access and release a shared resource, and the delay before access can occur (assuming messages are passed sequentially over a network). In the following, we assume only point-to-point messages (or, equivalently, count a multicast to N processes as N messages). • The centralized algorithm is simplest and also most efﬁcient. It requires only three messages to enter and leave a critical region: a request, a grant to enter, and a release to exit. • The distributed algorithm requires N − 1 request messages, one to each of the other processes, and an additional N − 1 grant messages, for a total of 2 · (N − 1). • With the token ring algorithm, the number is variable. If every process constantly wants to enter a critical region, then each token pass will result in one entry and exit, for an average of one message per critical region entered. At the other extreme, the token may sometimes circulate for hours without anyone being interested in it. In this case, the number of messages per entry into a critical region is unbounded. • In the decentralized case, we see that a request message needs to be sent to m coordinators, followed by a response message. However, it is possible that several attempts need to be made (for which we introduce the variable k). An exit requires sending a message to each of the m coordinators. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.4. ELECTION ALGORITHMS 329 The delay from the moment a process needs to enter a critical region until its actual entry also varies. For a worst-case analysis, we assume that messages are sent one after the other (i.e., there are never two or more messages in transit at the same time), and that message transfer time is roughly the same everywhere. Delay can then be expressed in message transfer time units, or simply MTTU. Under these assumptions, when the time using a resource is short, the dominant factor in the delay is determined by the total number of messages sent through the system before access can be granted. When resources are used for a long period of time, the dominant factor is waiting for everyone else to take their turn. In Figure 6.19 we show the former case. • It takes only two MTTUs to enter a critical region in the centralized case, caused by a request message and the subsequent grant message sent by the coordinator. • The distributed algorithm requires sending N − 1 request messages, and receiving another N − 1 grant messages, adding up to 2 · (N − 1) MTTUs. • For the token ring, the delay varies from 0 MTTU (in case the token had just arrived) to N − 1 (for when the token had just departed). • The decentralized case requires sending m messages to coordinators, and another m responses, but a process may need to go through k ≥ 1 attempts, adding up to 2 · m · k MTTUs. Virtually all algorithms suffer badly in the event of crashes. Special measures and additional complexity must be introduced to avoid having a crash bring down the entire system. It is somewhat ironic that distributed algorithms are generally more sensitive to crashes than centralized ones. In this sense, it should not come as a surprise that, indeed, centralized mutual exclusion is widely applied: it is simple to understand the behavior, and relatively easy to increase the fault tolerance of the centralized server. However, centralized solutions may suffer from scalability problems. 6.4 Election algorithms Many distributed algorithms require one process to act as coordinator, initiator, or otherwise perform some special role. In general, it does not matter which process takes on this special responsibility, but one of them has to do it. In this section we will look at algorithms for electing a coordinator (using this as a generic name for the special process). If all processes are exactly the same, with no distinguishing characteristics, there is no way to select one of them to be special. Consequently, we will assume that each process P has a unique identiﬁer id(P). In general, elec- tion algorithms attempt to locate the process with the highest identiﬁer and designate it as coordinator. The algorithms differ in the way they locate the coordinator. downloaded by VINHNT@TNU.EDU.VN DS 3.02 330 CHAPTER 6. COORDINATION Furthermore, we also assume that every process knows the identiﬁer of every other process. In other words, each process has complete knowledge of the process group in which a coordinator must be elected. What the processes do not know is which ones are currently up and which ones are currently down. The goal of an election algorithm is to ensure that when an election starts, it concludes with all processes agreeing on who the new coordinator is to be. There are many algorithms and variations, of which several important ones are discussed in the text books by Tel [2000] and Lynch [1996]. The bully algorithm A well-known solution for electing a coordinator is the bully algorithm de- vised by Garcia-Molina [1982]. In the following, we consider N processes {P0, . . . , PN−1} and let id(Pk) = k. When any process notices that the coordi- nator is no longer responding to requests, it initiates an election. A process, Pk, holds an election as follows: 1. Pk sends an ELECTION message to all processes with higher identiﬁers: Pk+1, Pk+2, . . . , PN−1. 2. If no one responds, Pk wins the election and becomes coordinator. 3. If one of the higher-ups answers, it takes over and Pk’s job is done. At any moment, a process can get an ELECTION message from one of its lower-numbered colleagues. When such a message arrives, the receiver sends an OK message back to the sender to indicate that he is alive and will take over. The receiver then holds an election, unless it is already holding one. Eventually, all processes give up but one, and that one is the new coordinator. It announces its victory by sending all processes a message telling them that starting immediately it is the new coordinator. If a process that was previously down comes back up, it holds an election. If it happens to be the highest-numbered process currently running, it will win the election and take over the coordinator’s job. Thus the biggest guy in town always wins, hence the name “bully algorithm.” In Figure 6.20 we see an example of how the bully algorithm works. The group consists of eight processes, with identiﬁers numbered from 0 to 7. Previously process P7 was the coordinator, but it has just crashed. Process P4 is the ﬁrst one to notice this, so it sends ELECTION messages to all the processes higher than it, namely P5, P6, and P7, as shown in Figure 6.20(a). Processes P5 and P6 both respond with OK, as shown in Figure 6.20(b). Upon getting the ﬁrst of these responses, P4 knows that its job is over, knowing that either one of P5 or P6 will take over and become coordinator. Process P4 just sits back and waits to see who the winner will be (although at this point it can make a pretty good guess). In Figure 6.20(c) both P5 and P6 hold elections, each one sending messages only to those processes with identiﬁers higher than itself. In Figure 6.20(d), DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.4. ELECTION ALGORITHMS 331 (a) (b) (c) (d) (e) Figure 6.20: The bully election algorithm. (a) Process 4 holds an election. (b) Processes 5 and 6 respond, telling 4 to stop. (c) Now 5 and 6 each hold an election. (d) Process 6 tells 5 to stop. (e) Process 6 wins and tells everyone. P6 tells P5 that it will take over. At this point P6 knows that P7 is dead and that it (P6) is the winner. If there is state information to be collected from disk or elsewhere to pick up where the old coordinator left off, P6 must now do what is needed. When it is ready to take over, it announces the takeover by sending a COORDINATOR message to all running processes. When P4 gets this message, it can now continue with the operation it was trying to do when it discovered that P7 was dead, but using P6 as the coordinator this time. In this way the failure of P7 is handled and the work can continue. downloaded by VINHNT@TNU.EDU.VN DS 3.02 332 CHAPTER 6. COORDINATION If process P7 is ever restarted, it will send all the others a COORDINATOR message and bully them into submission. A ring algorithm Consider the following election algorithm that is based on the use of a (logical) ring. Unlike some ring algorithms, this one does not use a token. We assume that each process knows who its successor is. When any process notices that the coordinator is not functioning, it builds an ELECTION message containing its own process identiﬁer and sends the message to its successor. If the successor is down, the sender skips over the successor and goes to the next member along the ring, or the one after that, until a running process is located. At each step along the way, the sender adds its own identiﬁer to the list in the message effectively making itself a candidate to be elected as coordinator. Eventually, the message gets back to the process that started it all. That pro- cess recognizes this event when it receives an incoming message containing its own identiﬁer. At that point, the message type is changed to COORDINATOR and circulated once again, this time to inform everyone else who the coordi- nator is (the list member with the highest identiﬁer) and who the members of the new ring are. When this message has circulated once, it is removed and everyone goes back to work. Figure 6.21: Election algorithm using a ring. The solid line shows the election messages initiated by P6; the dashed one those by P3. In Figure 6.21 we see what happens if two processes, P3 and P6, discover simultaneously that the previous coordinator, process P7, has crashed. Each of these builds an ELECTION message and each of them starts circulating its message, independent of the other one. Eventually, both messages will go all the way around, and both P3 and P6 will convert them into COORDINATOR messages, with exactly the same members and in the same order. When both have gone around again, both will be removed. It does no harm to have extra messages circulating; at worst it consumes a little bandwidth, but this is not considered wasteful. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.4. ELECTION ALGORITHMS 333 Elections in wireless environments Traditional election algorithms are generally based on assumptions that are not realistic in wireless environments. For example, they assume that message passing is reliable and that the topology of the network does not change. These assumptions are false in most wireless environments, especially those for mobile ad hoc networks. Only few protocols for elections have been developed that work in ad hoc networks. Vasudevan et al. [2004] propose a solution that can handle failing nodes and partitioning networks. An important property of their solution is that the best leader can be elected rather than just a random one as was more or less the case in the previously discussed solutions. Their protocol works as follows. To simplify our discussion, we concentrate only on ad hoc networks and ignore that nodes can move. Consider a wireless ad hoc network. To elect a leader, any node in the network, called the source, can initiate an election by sending an ELECTION message to its immediate neighbors (i.e., the nodes in its range). When a node receives an ELECTION for the ﬁrst time, it designates the sender as its parent, and subsequently sends out an ELECTION message to all its immediate neighbors, except for the parent. When a node receives an ELECTION message from a node other than its parent, it merely acknowledges the receipt. When node R has designated node Q as its parent, it forwards the ELECTION message to its immediate neighbors (excluding Q) and waits for acknowledgments to come in before acknowledging the ELECTION message from Q. This waiting has an important consequence. First, note that neighbors that have already selected a parent will immediately respond to R. More speciﬁcally, if all neighbors already have a parent, R is a leaf node and will be able to report back to Q quickly. In doing so, it will also report information such as its battery lifetime and other resource capacities. This information will later allow Q to compare R’s capacities to that of other downstream nodes, and select the best eligible node for leadership. Of course, Q had sent an ELECTION message only because its own parent P had done so as well. In turn, when Q eventually acknowledges the ELECTION message previously sent by P, it will pass the most eligible node to P as well. In this way, the source will eventually get to know which node is best to be selected as leader, after which it will broadcast this information to all other nodes. This process is illustrated in Figure 6.22. Nodes have been labeled a to j, along with their capacity. Node a initiates an election by broadcasting an ELECTION message to nodes b and j, as shown in Figure 6.22(b) After that step, ELECTION messages are propagated to all nodes, ending with the situation shown in Figure 6.22(e), where we have omitted the last broadcast by nodes f and i. From there on, each node reports to its parent the node with the best capacity, as shown in Figure 6.22(f). For example, when node g downloaded by VINHNT@TNU.EDU.VN DS 3.02 334 CHAPTER 6. COORDINATION (a) (b) (c) (d) (e) (f) Figure 6.22: Election algorithm in a wireless network, with node a as the source. (a) Initial network. (b)–(e) The build-tree phase (last broadcast step by nodes f and i not shown). (f) Reporting of best node to source. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.4. ELECTION ALGORITHMS 335 receives the acknowledgments from its children e and h, it will notice that h is the best node, propagating [h, 8] to its own parent, node b. In the end, the source will note that h is the best leader and will broadcast this information to all other nodes. When multiple elections are initiated, each node will decide to join only one election. To this end, each source tags its ELECTION message with a unique identiﬁer. Nodes will participate only in the election with the highest identiﬁer, stopping any running participation in other elections. With some minor adjustments, the protocol can be shown to operate also when the network partitions, and when nodes join and leave. The details can be found in Vasudevan et al. [2004]. Elections in large-scale systems Many leader-election algorithms apply to only relatively small distributed systems. Moreover, algorithms often concentrate on the selection of only a single node. There are situations when several nodes should actually be selected, such as in the case of super peers in peer-to-peer networks, which we discussed in Section 2.3. In this section, we concentrate speciﬁcally on the problem of selecting super peers. The following requirements need to be met for super-peer selection (see also [Lo et al., 2005]): 1. Normal nodes should have low-latency access to super peers. 2. Super peers should be evenly distributed across the overlay network. 3. There should be a predeﬁned portion of super peers relative to the total number of nodes in the overlay network. 4. Each super peer should not need to serve more than a ﬁxed number of normal nodes. Fortunately, these requirements are relatively easy to meet in most peer-to- peer systems, given the fact that the overlay network is either structured (as in DHT-based systems), or randomly unstructured (as, for example, can be realized with gossip-based solutions). Let us take a look at solutions proposed by Lo et al. [2005]. In the case of DHT-based systems, the basic idea is to reserve a fraction of the identiﬁer space for super peers. In a DHT-based system, each node receives a random and uniformly assigned m-bit identiﬁer. Now suppose we reserve the ﬁrst (i.e., leftmost) k bits to identify super peers. For example, if we need N superpeers, then the ﬁrst ⌈log2(N)⌉ bits of any key can be used to identify these nodes. To explain, assume we have a (small) Chord system with m = 8 and k = 3. When looking up the node responsible for a speciﬁc key K, we can ﬁrst decide to route the lookup request to the node responsible for the pattern downloaded by VINHNT@TNU.EDU.VN DS 3.02 336 CHAPTER 6. COORDINATION K ∧ 11100000 which is then treated as the superpeer.1 Note that each node with identiﬁer ID can check whether it is a super peer by looking up ID ∧ 11100000 to see if this request is routed to itself. Provided node identiﬁers are uniformly assigned to nodes, it can be seen that with a total of N nodes the number of super peers is, on average, equal to 2k−m N. A completely different approach is based on positioning nodes in an m- dimensional geometric space. In this case, assume we need to place N super peers evenly throughout the overlay. The basic idea is simple: a total of N tokens are spread across N randomly chosen nodes. No node can hold more than one token. Each token represents a repelling force by which another token is inclined to move away. The net effect is that if all tokens exert the same repulsion force, they will move away from each other and spread themselves evenly in the geometric space. This approach requires that nodes holding a token learn about other tokens. To this end, we can use a gossiping protocol by which a token’s force is disseminated throughout the network. If a node discovers that the total forces that are acting on it exceed a threshold, it will move the token in the direction of the combined forces, as shown in Figure 6.23. When a token is held by a node for a given amount of time, that node will promote itself to superpeer. Figure 6.23: Moving tokens in a two-dimensional space using repulsion forces. 6.5 Location systems When looking at very large distributed systems that are dispersed across a wide-area network, it is often necessary to take proximity into account. Just imagine a distributed system organized as an overlay network in which two processes are neighbors in the overlay network, but are actually placed far apart in the underlying network. If these two processes communicate a lot, it may have been better to ensure that they are also physically placed in each 1We use the binary operator “∧” to denote a bitwise and. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.5. LOCATION SYSTEMS 337 other’s proximity. In this section, we take a look at location-based techniques to coordinate the placement of processes and their communication. GPS: Global Positioning System Let us start by considering how to determine your geographical position anywhere on Earth. This positioning problem is by itself solved through a highly speciﬁc, dedicated distributed system, namely GPS, which is an acronym for Global Positioning System. GPS is a satellite-based distributed system that was launched in 1978. Although it initially was used mainly for military applications, it by now has found its way to many civilian applications, notably for trafﬁc navigation. However, many more application domains exist. For example, modern smartphones now allow owners to track each other’s position. This principle can easily be applied to tracking other things as well, including pets, children, cars, boats, and so on. GPS uses up to 72 satellites each circulating in an orbit at a height of approximately 20,000 km. Each satellite has up to four atomic clocks, which are regularly calibrated from special stations on Earth. A satellite continuously broadcasts its position, and time stamps each message with its local time. This broadcasting allows every receiver on Earth to accurately compute its own position using, in principle, only four satellites. To explain, let us ﬁrst assume that all clocks, including the receiver’s, are synchronized. In order to compute a position, consider ﬁrst the two-dimensional case, as shown in Figure 6.24, in which three satellites are drawn, along with the circles representing points at the same distance from each respective satellite. We see that the intersection of the three circles is a unique point. Figure 6.24: Computing a node’s position in a two-dimensional space. downloaded by VINHNT@TNU.EDU.VN DS 3.02 338 CHAPTER 6. COORDINATION This principle of intersecting circles can be expanded to three dimensions, meaning that we need to know the distance to four satellites to determine the longitude, latitude, and altitude of a receiver on Earth. This positioning is all fairly straightforward, but determining the distance to a satellite becomes complicated when we move from theory to practice. There are at least two important real-world facts that we need to take into account: 1. It takes a while before data on a satellite’s position reaches the receiver. 2. The receiver’s clock is generally not in sync with that of a satellite. Assume that the timestamp from a satellite is completely accurate. Let ∆r denote the deviation of the receiver’s clock from the actual time. When a message is received from satellite Si with timestamp Ti, then the measured delay ∆i by the receiver consists of two components: the actual delay, along with its own deviation: ∆i = (Tnow − Ti) + ∆r where Tnow is the actual current time. As signals travel with the speed of light, c, the measured distance ˜di to satellite Si is equal to c · ∆i. With di = c · (Tnow − Ti) being the real distance between the receiver and satellite Si, the measured distance can be rewritten to ˜di = di + c · ∆r. The real distance is now computed as: ˜di − c · ∆r = √(xi − xr)2 + (yi − yr)2 + (zi − zr)2 where xi, yi, and zi denote the coordinates of satellite Si. What we see now is a system of quadratic equations with four unknowns (xr, yr, zr, and also ∆r). We thus need four reference points (i.e., satellites) to ﬁnd a unique solution that will also give us ∆r. A GPS measurement will thus also give an account of the actual time. So far, we have assumed that measurements are perfectly accurate. Of course, they are not. There are many sources of errors, starting with the fact that the atomic clocks in the satellites are not always in perfect sync, the position of a satellite is not known precisely, the receiver’s clock has a ﬁnite accuracy, the signal propagation speed is not constant (as signals appear to slow down when entering, e.g., the ionosphere), and so on. On average, this leads to an error of some 5–10 meters. Special modulation techniques, as well as special receivers, are needed to improve accuracy. Using so-called differential GPS, by which corrective information is sent through wide-area links, accuracy can be further improved. More information can be found in [LaMarca and de Lara, 2008], as well as an excellent overview by Zogg [2002]. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.5. LOCATION SYSTEMS 339 When GPS is not an option A major drawback of GPS is that it can generally not be used indoors. For that purpose, other techniques are necessary. An increasingly popular technique is to make use of the numerous WiFi access points available. The basic idea is simple: if we have a database of known access points along with their coordinates, and we can estimate our distance to an access point, then with only three detected access points, we should be able to compute our position. Of course, it really is not that simple at all. A major problem is determining the coordinates of an access point. A popular approach is to do this through war driving: using a WiFi-enabled device along with a GPS receiver, someone drives or walks through an area and records observed access points. An access point can be identiﬁed through its SSID or its MAC-level network address. An access point AP should be detected at several locations before its coordinates can be estimated. A simple method is to compute the centroid: assume we have detected AP at N different locations {⃗x1, ⃗x2, . . . , ⃗xN}, where each location ⃗xi consists of a (latitude, longitude)-pair as provided by the GPS receiver. We then simply estimate AP’s location ⃗xAP as ⃗xAP = ∑N i=1 ⃗xi N . Accuracy can be improved by taking the observed signal strength into account, and giving more weight to a location with relatively high observed signal strength than to a location where only a weak signal was detected. In the end, we obtain an estimation of the coordinates of the access point. The accuracy of this estimation is strongly inﬂuenced by: • the accuracy of each GPS detection point ⃗xi • the fact that an access point has a nonuniform transmission range • the number of sampled detection points N. Studies show that estimates of the coordinates of an access point may be tens of meters off from the actual location (see, e.g., Kim et al. [2006a] or Tsui et al. [2010]). Moreover, access points come and go at a relatively high rate. Nevertheless, locating and positioning access points is widely popular, exempliﬁed by the open-access Wigle database which is populated through crowd sourcing.2 Logical positioning of nodes Instead of trying to ﬁnd the absolute location of a node in a distributed system, an alternative is to use a logical, proximity-based location. In geometric 2See wigle.net. downloaded by VINHNT@TNU.EDU.VN DS 3.02 340 CHAPTER 6. COORDINATION overlay networks each node is given a position in an m-dimensional geometric space, such that the distance between two nodes in that space reﬂects a real- world performance metric. Computing such a position is the core business of a Network Coordinates System, or simply NCS, which are surveyed by Donnet et al. [2010]. The simplest, and most applied example, is where distance corresponds to internode latency. In other words, given two nodes P and Q, then the distance ˆd(P, Q) reﬂects how long it would take for a message to travel from P to Q and vice versa. We use the notation ˆd to denote distance in a system where nodes have been assigned coordinates. There are many applications of geometric overlay networks. Consider the situation where a Web site at server O has been replicated to multiple servers S1, . . . , SN on the Internet. When a client C requests a page from O, the latter may decide to redirect that request to the server closest to C, that is, the one that will give the best response time. If the geometric location of C is known, as well as those of each replica server, O can then simply pick that server Si for which ˆd(C, Si) is minimal. Note that such a selection requires only local processing at O. In other words, there is, for example, no need to sample all the latencies between C and each of the replica servers. Another example is optimal replica placement. Consider again a Web site that has gathered the positions of its clients. If the site were to replicate its content to N servers, it can compute the N best positions where to place replicas such that the average client-to-replica response time is minimal. Performing such computations is almost trivially feasible if clients and servers have geometric positions that reﬂect internode latencies. As a last example, consider position-based routing (see, e.g., [Popescu et al., 2012] or [Bilal et al., 2013]). In such schemes, a message is forwarded to its destination using only positioning information. For example, a naive routing algorithm to let each node forward a message to the neighbor closest to the destination. Although it can be easily shown that this speciﬁc algorithm need not converge, it illustrates that only local information is used to take a decision. There is no need to propagate link information or such to all nodes in the network, as is the case with conventional routing algorithms. Centralized positioning Positioning a node in an m-dimensional geometric space requires m + 1 distance measures to nodes with known positions. Assuming that node P wants to compute its own position, it contacts three other nodes with known positions and measures its distance to each of them. Contacting only one node would tell P about the circle it is located on; contacting only two nodes would tell it about the position of the intersection of two circles (which generally consists of two points); a third node would subsequently allow P to compute its actual location. DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.5. LOCATION SYSTEMS 341 Node P can compute its own coordinates (xP, yP) by solving the three quadratic equations with the two unknowns xP and yP: ˜di = √(xi − xP)2 + (yi − yP)2 (i = 1, 2, 3) Here, we use ˜d to denote measured, or estimated distance. As said, ˜di generally corresponds to measuring the latency between P and the node at (xi, yi). This latency can be estimated as being half the round-trip delay, but it should be clear that its value will be different over time. The effect is a different positioning whenever P would want to recompute its position. Moreover, if other nodes would use P’s current position to compute their own coordinates, then it should be clear that the error in positioning P will affect the accuracy of the positioning of other nodes. It should also be clear that measured distances between a set of nodes will generally not even be consistent. For example, assume we are computing distances in a one-dimensional space as shown in Figure 6.25. In this example, we see that although R measures its distance to Q as 2.0, and ˜d(P, Q) has been measured to be 1.0, when R measures ˜d(P, R) it ﬁnds 2.8, which is clearly inconsistent with the other two measurements. Figure 6.25: Inconsistent distance measurements in a one-dimensional space. Figure 6.25 also suggests how this situation can be improved. In our simple example, we could solve the inconsistencies by merely computing positions in a two-dimensional space. This by itself, however, is not a general solution when dealing with many measurements. In fact, considering that Internet latency measurements may violate the triangle inequality, it is generally impossible to resolve inconsistencies completely. The triangle inequality states that in a geometric space, for any arbitrary three nodes P, Q, and R it must always be true that d(P, R) ≤ d(P, Q) + d(Q, R). There are various ways to approach these issues. One common approach, proposed by Ng and Zhang [2002], is to use N special nodes L1, . . . , LN, known as landmarks. Landmarks measure their pairwise latencies ˜d(Li, Lj) and subsequently let a central node compute the coordinates for each landmark. To this end, the central node seeks to minimize the following aggregated error downloaded by VINHNT@TNU.EDU.VN DS 3.02 342 CHAPTER 6. COORDINATION function: N ∑ i=1 N ∑ j=i+1 ( ˜d(Li, Lj) − ˆd(Li, Lj) ˜d(Li, Lj) )2 where, again, ˆd(Li, Lj) corresponds to the distance after nodes Li and Lj have been positioned. The hidden parameter in minimizing the aggregated error function is the dimension m. Obviously, we have that N > m, but nothing prevents us from choosing a value for m that is much smaller than N. In that case, a node P measures its distance to each of the N landmarks and computes its coordinates by minimizing N ∑ i=1 ( ˜d(Li, P) − ˆd(Li, P) ˜d(Li, P) )2 As it turns out, with well-chosen landmarks, m can be as small as 6 or 7, with ˆd(P, Q) being no more than a factor 2 different from the actual latency d(P, Q) for arbitrary nodes P and Q [Szymaniak et al., 2004; 2008]. Decentralized positioning Another way to tackle this problem is to view the collection of nodes as a huge system in which nodes are attached to each other through springs. In this case, | ˜d(P, Q) − ˆd(P, Q)| indicates to what extent nodes P and Q are displaced relative to the situation in which the system of springs would be at rest. By letting each node (slightly) change its position, it can be shown that the system will eventually converge to an optimal organization in which the aggregated error is minimal. This approach is followed in Vivaldi, described in [Dabek et al., 2004a]. In a system with N nodes P1, . . . , PN, Vivaldi aims at minimizing the following aggregated error: N ∑ i=1 N ∑ j=1 | ˜d(Pi, Pj) − ˆd(Pi, Pj)|2 where ˜d(Pi, Pj) is the measured distance (i.e., latency) between nodes Pi and Pj, and ˆd(Pi, Pj) the distance computed from the network coordinates of each node. Let ⃗xi denote the coordinates of node Pi. In a situation that each node is placed in a geometric space, the force that node Pi exerts on node Pj is computed as: ⃗Fij = ( ˜d(Pi, Pj) − ˆd(Pi, Pj)) × u(⃗xi − ⃗xj) with u(⃗xi − ⃗xj) denoting the unit vector in the direction of ⃗xi − ⃗xj. In other words, if Fij > 0, node Pi will push Pj away from itself, and will otherwise pull it toward itself. Node Pi now repeatedly executes the following steps: DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.6. DISTRIBUTED EVENT MATCHING 343 1. Measure the latency ˜dij to node Pj, and also receive Pj’s coordinates ⃗xj. 2. Compute the error e = ˜d(Pi, Pj) − ˆd(Pi, Pj) 3. Compute the direction ⃗u = u(⃗xi − ⃗xj). 4. Compute the force vector Fij = e · ⃗u 5. Adjust own position by moving along the force vector: ⃗xi ← ⃗xi + δ · ⃗u. A crucial element is the choice of δ: too large and the system will oscillate; too small and convergence to a stable situation will take a long time. The trick is to have an adaptive value, which is large when the error is large as well, but small when only small adjustments are needed. Details can be found in [Dabek et al., 2004a]. 6.6 Distributed event matching As a ﬁnal subject concerning the coordination among processes, we consider distributed event matching. Event matching, or more precisely, notiﬁcation ﬁltering, is at the heart of publish-subscribe systems. The problem boils down to the following: • A process speciﬁes through a subscription S in which events it is inter- ested. • When a process publishes a notiﬁcation N on the occurrence of an event, the system needs to see if S matches N. • In the case of a match, the system should send the notiﬁcation N, possibly including the data associated with the event that took place, to the subscriber. As a consequence, we need to facilitate at least two things: (1) matching subscriptions against events, and (2) notifying a subscriber in the case of a match. The two can be separated, but this need not always be the case. In the following, we assume the existence of a function match(S, N) which returns true when subscription S matches the notiﬁcation N, and false otherwise. Centralized implementations A simple, naive implementation of event matching is to have a fully centralized server that handles all subscriptions and notiﬁcations. In such a scheme, a subscriber simply submits a subscription, which is subsequently stored. When a publisher submits a notiﬁcation, that notiﬁcation is checked against each and every subscription, and when a match is found, the notiﬁcation is copied and forwarded to the associated subscriber. Obviously, this is not a very scalable solution. Nevertheless, provided the matching can be done efﬁciently and the server itself has enough processing power, the solution is feasible for many cases. For example, using a centralized downloaded by VINHNT@TNU.EDU.VN DS 3.02 344 CHAPTER 6. COORDINATION server is the canonical solution for implementing Linda tuple spaces or Java Spaces. Likewise, many publish-subscribe systems that run within a single department or organization can be implemented through a central server. Important in these cases, is that the matching function can be implemented efﬁciently. In practice, this is often the case when dealing with topic-based ﬁltering: matching then resorts to checking for equality of attribute values. Note that a simple way to scale up a centralized implementation, is to de- terministically divide the work across multiple servers. A standard approach is to make use of two functions, as explained by Baldoni et al. [2009]: • a function sub2node(S), which takes a subscription S and maps it to a nonempty subset of servers • a function not2node(N), which takes a notiﬁcation N and maps it to a nonempty subset of servers. The servers to which sub2node(S) is mapped are called the rendezvous nodes for S. Likewise, sub2node(N) are the rendezvous nodes for N. The only constraint that needs to be satisﬁed, is that for any subscription S and matching notiﬁcation N, sub2node(S) ∩ not2node(N) ̸= ∅. In other words, there must be at least one server that can handle the subscription when there is a matching notiﬁcation. In practice, this constraint is satisﬁed by topic-based publish- subscribe systems by using a hashing function on the names of the topics. The idea of having a central server can be extended by distributing the matching across multiple servers and dividing the work. The servers, gener- ally referred to as brokers, are organized into an overlay network. The issue then becomes how to route notiﬁcations to the appropriate set of subscribers. Following Baldoni et al. [2009], we distinguish three different classes: (1) ﬂood- ing, (2) selective routing, and (3) gossip-based dissemination. An extensive survey on combining peer-to-peer networks and publish-subscribe systems is provided by Kermarrec and Triantaﬁllou [2013]. A straightforward way to make sure that notiﬁcations reach their sub- scribers, is to deploy broadcasting. There are essentially two approaches. First, we store each subscription at every broker, while publishing notiﬁcations only a single broker. The latter will handle identifying the matching subscriptions and subsequently copy and forward the notiﬁcation. The alternative is to store a subscription only at one broker while broadcasting notiﬁcations to all brokers. In that case, matching is distributed across the brokers which may lead to a more balanced workload among the brokers. Note 6.6 (Example: TIB/Rendezvous) Flooding notiﬁcations is used in TIB/Rendezvous, of which the basic archi- tecture is shown in Figure 6.26 [TIBCO]. In this approach, a notiﬁcation is a message tagged with a compound keyword describing its content, such as DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.6. DISTRIBUTED EVENT MATCHING 345 news.comp.os.books. A subscriber provides (parts of) a keyword, or indicating the messages it wants to receive, such as news.comp. ∗ .books. These keywords are said to indicate the subject of a message. Figure 6.26: The principle of a publish/subscribe system as implemented in TIB/Rendezvous. Fundamental to its implementation is the use of broadcasting common in local-area networks, although it also uses more efﬁcient communication facilities when possible. For example, if it is known exactly where a subscriber resides, point-to-point messages will generally be used. Each host on such a network will run a rendezvous daemon, which takes care that messages are sent and delivered according to their subject. Whenever a message is published, it is multicast to each host on the network running a rendezvous daemon. Typically, multicasting is implemented using the facilities offered by the underlying network, such as IP-multicasting or hardware broadcasting. Processes that subscribe to a subject pass their subscription to their local daemon. The daemon constructs a table of (process, subject), entries and whenever a message on subject S arrives, the daemon simply checks in its table for local subscribers, and forwards the message to each one. If there are no subscribers for S, the message is discarded immediately. When using multicasting as is done in TIB/Rendezvous, there is no reason why subscriptions cannot be elaborate and be more than string comparison as is currently the case. The crucial observation here is that because messages are forwarded to every node anyway, the potentially complex matching of published data against subscriptions can be done entirely locally without further network communication. needed. When systems become big, ﬂooding is not the best way to go, if even possible. Instead, routing notiﬁcations across the overlay network of brokers may be necessary. This is typically the way to go in information-centric networking, which makes use of name-based routing [Ahlgren et al., 2012; Xylomenos et al., 2014]. Name-based routing is a special case of selective notiﬁcation routing. Crucial in this setup is that brokers can take routing decisions by considering the content of a notiﬁcation message. More precisely, downloaded by VINHNT@TNU.EDU.VN DS 3.02 346 CHAPTER 6. COORDINATION it is assumed that each notiﬁcation carries enough information that can be used to cut-off routes for which it is known that they do not lead to its subscribers. A practical approach toward selective routing is proposed by Carzaniga et al. [2004]. Consider a publish-subscribe system consisting of N brokers to which clients (i.e., applications) can send subscriptions and retrieve noti- ﬁcations. Carzaniga et al. propose a two-layered routing scheme in which the lowest layer consists of a shared broadcast tree connecting the N brokers. There are various ways for setting up such a tree, ranging from network- level multicast support to application-level multicast trees as we discussed in Chapter 4. Here, we also assume that such a tree has been set up with the N brokers as end nodes, along with a collection of intermediate nodes forming routers. Note that the distinction between a server and a router is only a logical one: a single machine may host both kinds of processes. Figure 6.27: Naive content-based routing. Every broker broadcasts its subscriptions to all other brokers. As a result, every broker will be able to compile a list of (subject, destination) pairs. Then, whenever a process publishes a notiﬁcation N, its associated broker prepends the destination brokers to that message. When the message reaches a router, the latter can use the list to decide on the paths that the message should follow, as shown in Figure 6.27. We can reﬁne the capabilities of routers for deciding where to forward notiﬁcations to. To that end, each broker broadcasts its subscription across the network so that routers can compose routing ﬁlters. For example, assume that node 3 in Figure 6.27 subscribes to notiﬁcations for which an attribute a lies in the range [0, 3], but that node 4 wants messages with a ∈ [2, 5]. In this case, router R2 will create a routing ﬁlter as a table with an entry for each of its outgoing links (in this case three: one to node 3, one to node 4, and one toward router R1), as shown in Figure 6.28. More interesting is what happens at router R1. In this example, the subscriptions from nodes 3 and 4 dictate that any notiﬁcation with a lying in the interval [0, 3] ∪ [2, 5] = [0, 5] should be forwarded along the path to router R2, and this is precisely the information that R1 will store in its table. It is DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.6. DISTRIBUTED EVENT MATCHING 347 Interface Filter To node 3 a ∈ [0, 3] To node 4 a ∈ [2, 5] Toward router R1 (unspeciﬁed) Figure 6.28: A partially ﬁlled routing table. not difﬁcult to imagine that more intricate subscription compositions can be supported. This simple example also illustrates that whenever a node leaves the system, or when it is no longer interested in speciﬁc notiﬁcations, it should cancel its subscription and essentially broadcast this information to all routers. This cancellation, in turn, may lead to adjusting various routing ﬁlters. Late adjustments will at worst lead to unnecessary trafﬁc as notiﬁcations may be forwarded along paths for which there are no longer subscribers. Nevertheless, timely adjustments are needed to keep performance at an acceptable level. The last type of distributed event matching is based on gossiping. The basic idea is that subscribers interested in the same notiﬁcations form their own overlay network (which is constructed through gossiping), so that once a notiﬁcation is published, it merely needs to be routed to the appropriate overlay. For the latter, a random walk can be deployed. This approach is following in TERA [Baldoni et al., 2007]. As an alternative, in PolderCast [Setty et al., 2012] a publisher ﬁrst joins the overlay of subscribers before ﬂooding its notiﬁcation. The subscriber overlay is built per topic and constitutes a ring with shortcuts to facilitate efﬁcient dissemination of a notiﬁcation. Note 6.7 (Advanced: Gossiping for content-based event matching) A more sophisticated approach toward combining gossiping and event matching is followed in Sub-2-Sub [Voulgaris et al., 2006]. Consider a publish-subscribe system in which data items can be described by means of N attributes a1, . . . , aN whose value can be directly mapped to a ﬂoating-point number. Such values include, for example, ﬂoats, integers, enumerations, booleans, and strings. A subscription S takes the form of a tuple of (attribute, value/range) pairs, such as S = ⟨a1 → 3.0, a4 → [0.0, 0.5)⟩ In this example, S speciﬁes that a1 should be equal to 3.0, and a4 should lie in the interval [0.0, 0.5). Other attributes are allowed to take on any value. For clarity, assume that every node i enters only one subscription Si. Note that each subscription Si actually speciﬁes a subset Si in the N- dimensional space of ﬂoating-point numbers. Such a subset is also called a hyperspace. For the system as a whole, notiﬁcations that fall in the union S = ∪Si of these hyperspaces are the only ones of interest. The whole idea is to auto- matically partition S into M disjoint hyperspaces S1, . . . , SM such that each falls downloaded by VINHNT@TNU.EDU.VN DS 3.02 348 CHAPTER 6. COORDINATION completely in one of the subscription hyperspaces Si, and together they cover all subscriptions. More formally, we have that: (Sm ∩ Si ̸= ∅) ⇒ (Sm ⊆ Si) Sub-2-Sub keeps M minimal in the sense that there is no partitioning with fewer parts Sm. To this end, for each hyperspace Sm, it registers exactly those nodes i for which Sm ⊆ Si. In that case, when a notiﬁcation is published, the system need merely ﬁnd the Sm to which the associated event belongs, from which point it can forward the notiﬁcation to the appropriate nodes. To this end, nodes regularly exchange subscriptions through gossiping. If two nodes i and j notice that their respective subscriptions intersect, that is, Sij ≡ Si ∩ Sj ̸= ∅ they will record this fact and keep references to each other. If they discover a third node k with Sijk ≡ Sij ∩ Sk ̸= ∅, the three of them will connect to each other so that a notiﬁcation N from Sijk can be efﬁciently disseminated. Note that if Sij − Sijk ̸= ∅, nodes i and j will maintain their mutual references, but now associate it strictly with Sij − Sijk. In essence, what we are seeking is a means to cluster nodes into M different groups, such that nodes i and j belong to the same group if and only if their subscriptions Si and Sj intersect. Moreover, nodes in the same group should be organized into an overlay network that will allow efﬁcient dissemination of a data item in the hyperspace associated with that group. This situation for a single attribute is sketched in Figure 6.29. Figure 6.29: Grouping nodes for supporting range queries in a gossip- based publish-subscribe system. Here, we see a total of seven nodes in which the horizontal line for node i indicates its range of interest for the value of the single attribute. Also shown is the grouping of nodes into disjoint ranges of interests for values of the attribute. For example, nodes 3, 4, 7, and 10 will be grouped together representing the interval [16.5, 21.0]. Any data item with a value in this range should be disseminated to only these four nodes. To construct these groups, the nodes are organized into a gossip-based un- structured network. Each node maintains a list of references to other neighbors (i.e., a partial view), which it periodically exchanges with one of its neighbors. Such an exchange will allow a node to learn about random other nodes in the sys- DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.7. GOSSIP-BASED COORDINATION 349 tem. Every node keeps track of the nodes it discovers with overlapping interests (i.e., with an intersecting subscription). At a certain moment, every node i will generally have references to other nodes with overlapping interests. As part of exchanging information with a node j, node i orders these nodes by their identiﬁers and selects the one with the lowest identiﬁer i1 > j, such that its subscription overlaps with that of node j, that is, Sj,i1 ≡ Si1 ∩ Sj ̸= ∅. The next one to be selected is i2 > i1 such that its subscription also overlaps with that of j, but only if it contains elements not yet covered by node i1. In other words, we should have that Sj,i1,i2 ≡ (Si2 − Sj,i1 ) ∩ Sj ̸= ∅. This process is repeated until all nodes that have an overlapping interest with node i have been inspected, leading to an ordered list i1 < i2 < · · · < in. Note that a node ik is in this list because it covers a region R of common interest to node i and j not yet jointly covered by nodes with a lower identiﬁer than ik. In effect, node ik is the ﬁrst node that node j should forward a notiﬁcation to that falls in this unique region R. This procedure can be expanded to let node i construct a bidirectional ring. Such a ring is also shown in Figure 6.29. Whenever a notiﬁcation N is published, it is disseminated as quickly as possible to any node that is interested in it. As it turns out, with the information available at every node ﬁnding a node i interested in N is simple. From there on, node i need simply forward N along the ring of subscribers for the particular range that N falls into. To speed up dissemination, shortcuts are maintained for each ring as well. 6.7 Gossip-based coordination As a ﬁnal topic in coordination, we take a look at a few important examples in which gossiping is deployed. In the following, we look at aggregation, large-scale peer sampling, and overlay construction, respectively. Aggregation Let us take a look at some interesting applications of epidemic protocols. We already mentioned spreading updates, which is perhaps the most widely- deployed application. In the same light, gossiping can be used to discover nodes that have a few outgoing wide-area links, to subsequently apply direc- tional gossiping. Another interesting application area is simply collecting, or actually aggre- gating information [Jelasity et al., 2005]. Consider the following information exchange. Every node Pi initially chooses an arbitrary number, say vi. When node Pi contacts node Pj, they each update their value as: vi, vj ← (vi + vj)/2 downloaded by VINHNT@TNU.EDU.VN DS 3.02 350 CHAPTER 6. COORDINATION Obviously, after this exchange, both Pi and Pj will have the same value. In fact, it is not difﬁcult to see that eventually all nodes will have the same value, namely the average of all initial values. Propagation speed is again exponential. What use does computing the average have? Consider the situation that all nodes Pi have set vi to zero, except for P1 who has set v1 to 1: vi ← {1 if i = 1 0 otherwise If there are N nodes, then eventually each node will compute the average, which is 1/N. As a consequence, every node Pi can estimate the size of the system as being 1/vi. Computing the average may prove to be difﬁcult when nodes regularly join and leave the system. One practical solution to this problem is to introduce epochs. Assuming that node P1 is stable, it simply starts a new epoch now and then. When node Pi sees a new epoch for the ﬁrst time, it resets its own variable vi to zero and starts computing the average again. Of course, other results can also be computed. For example, instead of having a ﬁxed node such as P1 start the computation of the average, we can easily pick a random node as follows. Every node Pi initially sets vi to a random number from the same interval, say (0, 1], and also stores it permanently as mi. Upon an exchange between nodes Pi and Pj, each change their value to: vi, vj ← max{vi, vj} Each node Pi for which mi < vi will lose the competition for being the initiator in starting the computation of the average. In the end, there will be a single winner. Of course, although it is easy to conclude that a node has lost, it is much more difﬁcult to decide that it has won, as it remains uncertain whether all results have come in. The solution to this problem is to be optimistic: a node always assumes it is the winner until proven otherwise. At that point, it simply resets the variable it is using for computing the average to zero. Note that by now, several different computations (in our example computing a maximum and computing an average) may be executing simultaneously. A peer-sampling service An important aspect in epidemic protocols is the ability of a node P to choose another node Q at random from all available nodes in the network. When giving the matter some thought, we may actually have a serious problem: if the network consists of thousands of nodes, how can P ever pick one of these nodes at random without having a complete overview of the network? For smaller networks, one could often resort to a central service that had DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.7. GOSSIP-BASED COORDINATION 351 registered every participating node. Obviously, this approach can never scale to large networks. A solution is to construct a fully decentralized peer-sampling service, or PSS for short. As it turns out, and somewhat counter-intuitive, a PSS can be built using an epidemic protocol. As explored by Jelasity et al. [2007], each node maintains a list of c neighbors, where, ideally, each of these neighbors represents a randomly chosen live node from the current set of nodes. This list of neighbors is also referred to as a partial view. There are many ways to construct such a partial view. In their solution, it is assumed that nodes regularly exchange entries from their partial view. Each entry identiﬁes another node in the network, and has an associated age that indicates how old the reference to that node is. Two threads are used, as shown in Figure 6.30. 1 selectPeer(&Q); 2 selectToSend(&bufs); 3 sendTo(Q, bufs); 4 5 receiveFrom(Q, &bufr); 6 selectToKeep(p_view, bufr); −→ ←− 1 2 3 receiveFromAny(&P, &bufr); 4 selectToSend(&bufs); 5 sendTo(P, bufs); 6 selectToKeep(p_view, bufr); (a) (b) Figure 6.30: Communication between the (a) active and (b) passive thread in a peer-sampling service. The different selection operations are speciﬁed as follows: • selectPeer: Randomly select a neighbor from the local partial view • selectToSend: Select some other entries from the partial view, and add to the list intended for the selected neighbor. • selectToKeep: Add received entries to partial view, remove repeated items, and shrink view to c items. The active thread takes the initiative to communicate with another node. It selects that node from its current partial view. It continues by constructing a list containing c/2 + 1 entries, including an entry identifying itself. The other entries are taken from the current partial view. After sending the list to the selected neighbor, it waits for a response. That neighbor, in the meantime, will also have constructed a list by means of the passive thread shown in Figure 6.30(b) whose activities strongly resem- ble that of the active thread. The crucial point is the construction of a new partial view. This view, for contacting as well as for the contacted peer, will contain exactly c entries, part of which will come from the received list. In essence, there are two ways to construct the new view. First, the two nodes may decide to discard the entries that they had sent to each other. Effectively, this means that they will swap downloaded by VINHNT@TNU.EDU.VN DS 3.02 352 CHAPTER 6. COORDINATION part of their original views. The second approach is to discard as many old entries as possible (meaning, in practice, that after every gossiping round, the age of each entry in every partial view is incremented by one). As it turns out, as long as peers regularly run the exchange algorithm just described, selecting a random peer from a thus dynamically changing partial view, is indistinguishable from randomly selecting a peer from the entire network. Of course, selecting a peer from a partial view should occur at approximately the same frequency as the refreshing of partial views. We have thus constructed a fully decentralized gossip-based peer-sampling ser- vice. A simple, and often-used implementation of a peer-sampling service is Cyclon [Voulgaris et al., 2005]. Gossip-based overlay construction Although it would seem that structured and unstructured peer-to-peer systems form strict independent classes, this need actually not be case (see also Castro et al. [2005]). One key observation is that by carefully exchanging and selecting entries from partial views, it is possible to construct and maintain speciﬁc topologies of overlay networks. This topology management is achieved by adopting a two-layered approach, as shown in Figure 6.31. Figure 6.31: A two-layered approach for constructing and maintaining speciﬁc overlay topologies using techniques from unstructured peer-to-peer systems. The lowest layer constitutes an unstructured peer-to-peer system in which nodes periodically exchange entries of their partial views with the aim to provide a peer-sampling service. Accuracy in this case refers to the fact that the partial view should be ﬁlled with entries referring to randomly selected live nodes. The lowest layer passes its partial view to the higher layer, where an additional selection of entries takes place. This then leads to a second list of neighbors corresponding to the desired topology. Jelasity and Kermarrec [2006] propose to use a ranking function by which nodes are ordered according to some criterion relative to a given node. A simple ranking function is to order a set of nodes by increasing distance from a given node P. In that case, DS 3.02 downloaded by VINHNT@TNU.EDU.VN 6.8. SUMMARY 353 node P will gradually build up a list of its nearest neighbors, provided the lowest layer continues to pass randomly selected nodes. As an illustration, consider a logical grid of size N × N with a node placed on each point of the grid. Every node is required to maintain a list of c nearest neighbors, where the distance between a node at (a1, a2) and (b1, b2) is deﬁned as d1 + d2, with di = min(N − |ai − bi|, |ai − bi|). If the lowest layer periodically executes the protocol as outlined in Figure 6.30, the topology that will evolve is a torus, shown in Figure 6.31. Figure 6.32: Generating a speciﬁc overlay network using a two-layered un- structured peer-to-peer system (adapted with permission from [Jelasity and Babaoglu, 2006]). 6.8 Summary Strongly related to communication between processes is the issue of how processes in distributed systems synchronize. Synchronization is all about doing the right thing at the right time. A problem in distributed systems, and computer networks in general, is that there is no notion of a globally shared clock. In other words, processes on different machines have their own idea of what time it is. There are various way to synchronize clocks in a distributed system, but all methods are essentially based on exchanging clock values, while taking into account the time it takes to send and receive messages. Variations in communication delays and the way those variations are dealt with, largely determine the accuracy of clock synchronization algorithms. In many cases, knowing the absolute time is not necessary. What counts is that related events at different processes happen in the correct order. Lamport showed that by introducing a notion of logical clocks, it is possible for a collection of processes to reach global agreement on the correct ordering of events. In essence, each event e, such as sending or receiving a message, downloaded by VINHNT@TNU.EDU.VN DS 3.02 354 CHAPTER 6. COORDINATION is assigned a globally unique logical timestamp C(e) such that when event a happened before b, C(a) < C(b). Lamport timestamps can be extended to vector timestamps: if C(a) < C(b), we even know that event a causally preceded b. An important class of synchronization algorithms is that of distributed mutual exclusion. These algorithms ensure that in a distributed collection of processes, at most one process at a time has access to a shared resource. Distributed mutual exclusion can easily be achieved if we make use of a coordinator that keeps track of whose turn it is. Fully distributed algorithms also exist, but have the drawback that they are generally more susceptible to communication and process failures. Synchronization between processes often requires that one process acts as a coordinator. In those cases where the coordinator is not ﬁxed, it is necessary that processes in a distributed computation decide on who is going to be that coordinator. Such a decision is taken by means of election algorithms. Election algorithms are primarily used in cases where the coordinator can crash. However, they can also be applied for the selection of superpeers in peer-to-peer systems. Related to these synchronization problems is positioning nodes in a ge- ometric overlay. The basic idea is to assign each node coordinates from an m-dimensional space such that the geometric distance can be used as an accurate measure for the latency between two nodes. The method of assigning coordinates strongly resembles the one applied in determining the location and time in GPS. Particularly challenging when it comes to coordination is distributed event matching, which sits at the core of publish-subscribe systems. Relatively simple is the case when we have a central implementations where matching subscriptions against notiﬁcations can be done by essentially doing one-to-one comparisons. However, as soon as we aim at distributing the load, we are faced with the problem on deciding beforehand which node is responsible for which part of the subscriptions, without knowing what kind of notiﬁcations to expect. This is particularly problematic for content-based matching, which in the end, requires advanced ﬁltering techniques to route notiﬁcations to the proper subscribers. Finally, the most important aspect in gossip-based coordination is being able to select another peer randomly from an entire overlay. As it turns out, we can implement such a peer-sampling service using gossiping, by ensuring that the partial view is refreshed regularly and in a random way. Combining peer sampling with a selective replacement of entries in a partial view allows us to efﬁciently construct structured overlay networks. DS 3.02 downloaded by VINHNT@TNU.EDU.VN","libVersion":"0.3.2","langs":""}